{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10715d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = nib.load('/kaggle/input/datasetzip/not_skull_stripped/sub-BrainAge000019/anat/sub-BrainAge000019_T1w.nii/sub-BrainAge000019_T1w.nii')\n",
    "data = im.get_fdata()\n",
    "data.shape, im.affine, im.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/datasetzip/not_skull_stripped'\n",
    "label_path = list(Path(data_dir).glob(\"*.xlsx\"))\n",
    "label_ls = pd.read_excel(label_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ls = label_ls[(label_ls['subject_dx'] == 'control') & ((label_ls['subject_sex'] == 'm') | (label_ls['subject_sex'] == 'f'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30226f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sexes = label_ls[['subject_sex','subject_id']]\n",
    "sexes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sexes_dict = sexes.set_index('subject_id')['subject_sex'].to_dict()\n",
    "len(sexes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa44349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, im_dir, label_ls, transform=None):\n",
    "        self.im_dir = Path(im_dir)\n",
    "        self.label_ls = label_ls\n",
    "        self.transform = transform\n",
    "\n",
    "        # Gather valid image paths\n",
    "        self.im_filenames = [\n",
    "            path for path in sorted(self.im_dir.glob(\"*/*/*/*.nii\"))\n",
    "            if self._is_valid(path)\n",
    "        ]\n",
    "\n",
    "    def _is_valid(self, path):\n",
    "        subject_id = self.extract_subject_id(path)\n",
    "        if subject_id not in self.label_ls:\n",
    "            return False\n",
    "        try:\n",
    "            nib.load(path).get_fdata()  # just try loading (don't call get_fdata() yet)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def extract_subject_id(self, im_path):\n",
    "        for part in Path(im_path).parts:\n",
    "            if part.startswith(\"sub-BrainAge\"):\n",
    "                return part\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im_path = self.im_filenames[idx]\n",
    "        im = nib.load(im_path).get_fdata()\n",
    "\n",
    "        # Normalize\n",
    "        im = (im - np.min(im)) / (np.max(im) - np.min(im) + 1e-5)  # avoid divide-by-zero\n",
    "        im = im.astype(np.float32)\n",
    "\n",
    "        subject_id = self.extract_subject_id(im_path)\n",
    "        sex = self.label_ls.get(subject_id)\n",
    "        if sex == 'm':\n",
    "            label = 0\n",
    "        elif sex == 'f':\n",
    "            label = 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid label for subject {subject_id}: {sex}\")\n",
    "\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "\n",
    "        # add channel dim\n",
    "        return torch.as_tensor(im), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iid_client_split(dataset, num_client = 3,  val_ratio = 0.2):\n",
    "\n",
    "    client_datasets = []\n",
    "    sample_per_client = len(dataset) // num_client\n",
    "\n",
    "\n",
    "    for i in range(num_client):\n",
    "        start_idx = i * sample_per_client\n",
    "        end_idx = (i + 1) * sample_per_client if i < num_client - 1 else len(dataset)\n",
    "        indecies = list(range(start_idx, end_idx))\n",
    "\n",
    "        client_dataset = torch.utils.data.Subset(dataset, indecies)\n",
    "        train_dataset, val_dataset = random_split(client_dataset, [1 - val_ratio, val_ratio])\n",
    "\n",
    "        client_datasets.append((train_dataset, val_dataset))\n",
    "    return client_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    "torchdatasets = MRIDataset(data_dir, sexes_dict, pytorch_transforms)\n",
    "# check length\n",
    "print(len(torchdatasets))\n",
    "\n",
    "trainvalset, testset = random_split(torchdatasets, [0.8, 0.2], generator = torch.Generator().manual_seed(42))\n",
    "\n",
    "client_datasets = iid_client_split(trainvalset, num_client=NUM_CLIENTS, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "def load_datasets(partition_id: int):\n",
    "    print(\"load_datasets starting\")\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    "    torchdatasets = MRIDataset(data_dir, sexes_dict, pytorch_transforms)\n",
    "    trainvalset, testset = random_split(torchdatasets, [0.8, 0.2], generator = torch.Generator().manual_seed(42))\n",
    "    client_datasets = iid_client_split(trainvalset, num_client=NUM_CLIENTS, val_ratio=0.2)\n",
    "    train_set = client_datasets[partition_id][0]\n",
    "    val_set = client_datasets[partition_id][1]\n",
    "    test_set = testset\n",
    "    # Create train/val for each partition and wrap it into DataLoader\n",
    "    trainloader = DataLoader(\n",
    "        train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    valloader = DataLoader(val_set, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "    testloader = DataLoader(test_set, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "    print(\"load_datasets finished\")\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34461568",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            'conv1',\n",
    "            nn.Conv3d(num_input_features,\n",
    "                      bn_size * growth_rate,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=False))\n",
    "        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            'conv2',\n",
    "            nn.Conv3d(bn_size * growth_rate,\n",
    "                      growth_rate,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False))\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super().forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features,\n",
    "                                     p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,\n",
    "                 drop_rate):\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate,\n",
    "                                growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer{}'.format(i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super().__init__()\n",
    "        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            'conv',\n",
    "            nn.Conv3d(num_input_features,\n",
    "                      num_output_features,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=False))\n",
    "        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Densenet-BC model class\n",
    "    \n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_input_channels=1,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 growth_rate=32,\n",
    "                 block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64,\n",
    "                 bn_size=4,\n",
    "                 drop_rate=0,\n",
    "                 num_classes=1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = [('conv1',\n",
    "                          nn.Conv3d(n_input_channels,\n",
    "                                    num_init_features,\n",
    "                                    kernel_size=(conv1_t_size, 7, 7),\n",
    "                                    stride=(conv1_t_stride, 2, 2),\n",
    "                                    padding=(conv1_t_size // 2, 3, 3),\n",
    "                                    bias=False)),\n",
    "                         ('norm1', nn.BatchNorm3d(num_init_features)),\n",
    "                         ('relu1', nn.ReLU(inplace=True))]\n",
    "        if not no_max_pool:\n",
    "            self.features.append(\n",
    "                ('pool1', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)))\n",
    "        self.features = nn.Sequential(OrderedDict(self.features))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers,\n",
    "                                num_input_features=num_features,\n",
    "                                bn_size=bn_size,\n",
    "                                growth_rate=growth_rate,\n",
    "                                drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock{}'.format(i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition{}'.format(i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool3d(out,\n",
    "                                    output_size=(1, 1,\n",
    "                                                 1)).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=False, device = \"cuda:0\", lr = 1e-3):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    net.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    torch.set_grad_enabled(True)\n",
    "    net.train(True)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, acc_end = 0.0, 0.0\n",
    "        for (x, y) in trainloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.unsqueeze(1))\n",
    "            loss = criterion(output, y.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            acc = ((torch.sigmoid(output) > 0.5).float() == y.unsqueeze(1)).float().mean()\n",
    "            acc_end += acc\n",
    "        epoch_loss /= len(trainloader)\n",
    "        acc_end /= len(trainloader)\n",
    "        if verbose:\n",
    "            print(f\"Local epoch {epoch+1}: train loss {epoch_loss}, acc {acc_end} \")\n",
    "\n",
    "def test(net, testloader, device = \"cuda:0\") -> float | float:\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    net.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    losses, acc_end = 0.0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in testloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = net(x.unsqueeze(1))\n",
    "            loss = criterion(output, y.float().unsqueeze(1))\n",
    "            losses += loss.item()\n",
    "            acc = ((torch.sigmoid(output) > 0.5).float() == y.unsqueeze(1)).float().mean()\n",
    "            acc_end += acc\n",
    "        losses /= len(testloader)\n",
    "        acc_end /= len(testloader)\n",
    "    return losses, acc_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f622f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, valloader, testloader = load_datasets(partition_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DenseNet(num_init_features=32,growth_rate=16,block_config=(4, 8, 16, 12)).to(DEVICE)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(net, trainloader, 1, True, lr = 1e-4)\n",
    "    loss, accuracy = test(net, valloader)\n",
    "    print(f\"Epoch {epoch+1}: validation loss {loss}, accuracy {accuracy}\")\n",
    "    \n",
    "loss, accuracy = test(net, testloader)\n",
    "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Flower seperation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    print(\"Setting parameters\")\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.as_tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    print(\"Returning parameters\")\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fec452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, net, trainloader, valloader, partition_id):\n",
    "        print(f\"[Client {partition_id}] initializing\")\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.partition_id = partition_id\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net.to(self.device)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, 1, True, self.device, lr=7e-4)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader, self.device)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    # Load model\n",
    "    print(f\"Client {partition_id} loading model\")\n",
    "    net = DenseNet(num_init_features=32,growth_rate=16,block_config=(4, 8, 16, 12)).to(DEVICE)\n",
    "\n",
    "    # Load data (CIFAR-10)\n",
    "    # Note: each client gets a different trainloader/valloader, so each client\n",
    "    # will train and evaluate on their own unique data partition\n",
    "    # Read the node_config to fetch data partition associated to this node\n",
    "    print(f\"Client {partition_id} loading data partition\")\n",
    "    trainloader, valloader, _ = load_datasets(partition_id=partition_id)\n",
    "\n",
    "    # Create a single Flower client representing a single organization\n",
    "    # FlowerClient is a subclass of NumPyClient, so we need to call .to_client()\n",
    "    # to convert it to a subclass of `flwr.client.Client`\n",
    "    print(f\"Client {partition_id} starting\")\n",
    "    return FlowerClient(net, trainloader, valloader, partition_id).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd712c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    \"\"\"Construct components that set the ServerApp behaviour.\n",
    "\n",
    "    You can use settings in `context.run_config` to parameterize the\n",
    "    construction of all elements (e.g the strategy or the number of rounds)\n",
    "    wrapped in the returned ServerAppComponents object.\n",
    "    \"\"\"\n",
    "    print(\"Server is getting model parameters\")\n",
    "    ndarrays = get_parameters(net = DenseNet(num_init_features=32,growth_rate=16,block_config=(4, 8, 16, 12)).to(DEVICE))\n",
    "    parameters = ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    # Create FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=10,\n",
    "        min_evaluate_clients=5,\n",
    "        min_available_clients=10,\n",
    "        initial_parameters=parameters,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # <-- pass the metric aggregation function\n",
    "    )\n",
    "\n",
    "    # Configure the server for 5 rounds of training\n",
    "    config = ServerConfig(num_rounds=10)\n",
    "    print(\"Server starting\")\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create a new server instance with the updated FedAvg strategy\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
