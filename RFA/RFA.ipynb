
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11460111,"sourceType":"datasetVersion","datasetId":7180861}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport nibabel as nib\nfrom ipywidgets import interact\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom collections import Counter\nfrom pathlib import Path\nfrom torch.utils.data import random_split\nimport math\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport pytorch_lightning as pl\nimport os\nimport random\nfrom torch.utils.data import DataLoader, random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:12:39.103026Z","iopub.execute_input":"2025-05-03T13:12:39.103735Z","iopub.status.idle":"2025-05-03T13:12:48.559132Z","shell.execute_reply.started":"2025-05-03T13:12:39.103710Z","shell.execute_reply":"2025-05-03T13:12:48.558570Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# read label csv\ndata_dir = '/kaggle/input/mri-dataset/not_skull_stripped'\nlabel_path = list(Path(data_dir).glob(\"*.xlsx\"))\nlabel_ls = pd.read_excel(label_path[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:36:57.384281Z","iopub.execute_input":"2025-05-03T13:36:57.384875Z","iopub.status.idle":"2025-05-03T13:36:58.861981Z","shell.execute_reply.started":"2025-05-03T13:36:57.384850Z","shell.execute_reply":"2025-05-03T13:36:58.861408Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"label_ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:37:44.732518Z","iopub.execute_input":"2025-05-03T13:37:44.732816Z","iopub.status.idle":"2025-05-03T13:37:44.742856Z","shell.execute_reply.started":"2025-05-03T13:37:44.732796Z","shell.execute_reply":"2025-05-03T13:37:44.742041Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"      subject_age subject_dx subject_sex          subject_id    dataset_name\n0            55.4  pathology           m  sub-BrainAge000000   ABIDE/Caltech\n1            22.9  pathology           m  sub-BrainAge000001   ABIDE/Caltech\n2            39.2  pathology           m  sub-BrainAge000002   ABIDE/Caltech\n3            22.8  pathology           m  sub-BrainAge000003   ABIDE/Caltech\n4            34.6  pathology           f  sub-BrainAge000004   ABIDE/Caltech\n...           ...        ...         ...                 ...             ...\n23209          66    control           f  sub-BrainAge023209  RocklandSample\n23210          69    control           m  sub-BrainAge023210  RocklandSample\n23211          23    control           m  sub-BrainAge023211  RocklandSample\n23212          54    control           f  sub-BrainAge023212  RocklandSample\n23213          75    control           f  sub-BrainAge023213  RocklandSample\n\n[23214 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject_age</th>\n      <th>subject_dx</th>\n      <th>subject_sex</th>\n      <th>subject_id</th>\n      <th>dataset_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55.4</td>\n      <td>pathology</td>\n      <td>m</td>\n      <td>sub-BrainAge000000</td>\n      <td>ABIDE/Caltech</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22.9</td>\n      <td>pathology</td>\n      <td>m</td>\n      <td>sub-BrainAge000001</td>\n      <td>ABIDE/Caltech</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>39.2</td>\n      <td>pathology</td>\n      <td>m</td>\n      <td>sub-BrainAge000002</td>\n      <td>ABIDE/Caltech</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22.8</td>\n      <td>pathology</td>\n      <td>m</td>\n      <td>sub-BrainAge000003</td>\n      <td>ABIDE/Caltech</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>34.6</td>\n      <td>pathology</td>\n      <td>f</td>\n      <td>sub-BrainAge000004</td>\n      <td>ABIDE/Caltech</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23209</th>\n      <td>66</td>\n      <td>control</td>\n      <td>f</td>\n      <td>sub-BrainAge023209</td>\n      <td>RocklandSample</td>\n    </tr>\n    <tr>\n      <th>23210</th>\n      <td>69</td>\n      <td>control</td>\n      <td>m</td>\n      <td>sub-BrainAge023210</td>\n      <td>RocklandSample</td>\n    </tr>\n    <tr>\n      <th>23211</th>\n      <td>23</td>\n      <td>control</td>\n      <td>m</td>\n      <td>sub-BrainAge023211</td>\n      <td>RocklandSample</td>\n    </tr>\n    <tr>\n      <th>23212</th>\n      <td>54</td>\n      <td>control</td>\n      <td>f</td>\n      <td>sub-BrainAge023212</td>\n      <td>RocklandSample</td>\n    </tr>\n    <tr>\n      <th>23213</th>\n      <td>75</td>\n      <td>control</td>\n      <td>f</td>\n      <td>sub-BrainAge023213</td>\n      <td>RocklandSample</td>\n    </tr>\n  </tbody>\n</table>\n<p>23214 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"subject_paths = []\n\nfor subject_id in label_ls['subject_id']:\n    mri_folder = Path(data_dir) / subject_id / \"anat\"\n    \n    mri_file = list(mri_folder.glob(f\"{subject_id}_T1w.nii\"))\n    \n    if mri_file: \n        full_path = mri_file[0] / mri_file[0].name  \n        subject_paths.append(full_path.as_posix())\n    else:\n        subject_paths.append(None) \n\nlabel_ls['subject_path'] = subject_paths\nprint(label_ls.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:37:47.546796Z","iopub.execute_input":"2025-05-03T13:37:47.547076Z","iopub.status.idle":"2025-05-03T13:38:09.798962Z","shell.execute_reply.started":"2025-05-03T13:37:47.547055Z","shell.execute_reply":"2025-05-03T13:38:09.798037Z"}},"outputs":[{"name":"stdout","text":"  subject_age subject_dx subject_sex          subject_id   dataset_name  \\\n0        55.4  pathology           m  sub-BrainAge000000  ABIDE/Caltech   \n1        22.9  pathology           m  sub-BrainAge000001  ABIDE/Caltech   \n2        39.2  pathology           m  sub-BrainAge000002  ABIDE/Caltech   \n3        22.8  pathology           m  sub-BrainAge000003  ABIDE/Caltech   \n4        34.6  pathology           f  sub-BrainAge000004  ABIDE/Caltech   \n\n                                        subject_path  \n0  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n1  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n2  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n3  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n4  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"labels = label_ls[[\"subject_sex\", \"subject_id\", \"subject_path\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:38:46.825493Z","iopub.execute_input":"2025-05-03T13:38:46.826207Z","iopub.status.idle":"2025-05-03T13:38:46.832366Z","shell.execute_reply.started":"2025-05-03T13:38:46.826184Z","shell.execute_reply":"2025-05-03T13:38:46.831648Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"labels_data = labels.dropna(subset=['subject_sex', 'subject_path']).reset_index(drop=True)\nlabels_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:38:49.207633Z","iopub.execute_input":"2025-05-03T13:38:49.208148Z","iopub.status.idle":"2025-05-03T13:38:49.222372Z","shell.execute_reply.started":"2025-05-03T13:38:49.208126Z","shell.execute_reply":"2025-05-03T13:38:49.221641Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"      subject_sex          subject_id  \\\n0               m  sub-BrainAge000000   \n1               m  sub-BrainAge000001   \n2               m  sub-BrainAge000002   \n3               m  sub-BrainAge000003   \n4               f  sub-BrainAge000004   \n...           ...                 ...   \n10270           f  sub-BrainAge023209   \n10271           m  sub-BrainAge023210   \n10272           m  sub-BrainAge023211   \n10273           f  sub-BrainAge023212   \n10274           f  sub-BrainAge023213   \n\n                                            subject_path  \n0      /kaggle/input/mri-dataset/not_skull_stripped/s...  \n1      /kaggle/input/mri-dataset/not_skull_stripped/s...  \n2      /kaggle/input/mri-dataset/not_skull_stripped/s...  \n3      /kaggle/input/mri-dataset/not_skull_stripped/s...  \n4      /kaggle/input/mri-dataset/not_skull_stripped/s...  \n...                                                  ...  \n10270  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n10271  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n10272  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n10273  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n10274  /kaggle/input/mri-dataset/not_skull_stripped/s...  \n\n[10275 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject_sex</th>\n      <th>subject_id</th>\n      <th>subject_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>m</td>\n      <td>sub-BrainAge000000</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>m</td>\n      <td>sub-BrainAge000001</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>m</td>\n      <td>sub-BrainAge000002</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>m</td>\n      <td>sub-BrainAge000003</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f</td>\n      <td>sub-BrainAge000004</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10270</th>\n      <td>f</td>\n      <td>sub-BrainAge023209</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>10271</th>\n      <td>m</td>\n      <td>sub-BrainAge023210</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>10272</th>\n      <td>m</td>\n      <td>sub-BrainAge023211</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>10273</th>\n      <td>f</td>\n      <td>sub-BrainAge023212</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n    <tr>\n      <th>10274</th>\n      <td>f</td>\n      <td>sub-BrainAge023213</td>\n      <td>/kaggle/input/mri-dataset/not_skull_stripped/s...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10275 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"labels_data[\"subject_path\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:38:51.058387Z","iopub.execute_input":"2025-05-03T13:38:51.058679Z","iopub.status.idle":"2025-05-03T13:38:51.064132Z","shell.execute_reply.started":"2025-05-03T13:38:51.058657Z","shell.execute_reply":"2025-05-03T13:38:51.063432Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/mri-dataset/not_skull_stripped/sub-BrainAge000000/anat/sub-BrainAge000000_T1w.nii/sub-BrainAge000000_T1w.nii'"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"data = labels_data.sample(frac=1).reset_index(drop=True)\n\nnum_clients = 5\nclient_data = {i: {'subject_id': [], 'subject_sex': [], 'subject_path': []} for i in range(num_clients)}\n\nfor idx, row in data.iterrows():\n    client_id = idx % num_clients  \n    client_data[client_id]['subject_id'].append(row['subject_id'])\n    client_data[client_id]['subject_sex'].append(row['subject_sex'])\n    client_data[client_id]['subject_path'].append(row['subject_path'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:38:52.948054Z","iopub.execute_input":"2025-05-03T13:38:52.948621Z","iopub.status.idle":"2025-05-03T13:38:53.353850Z","shell.execute_reply.started":"2025-05-03T13:38:52.948595Z","shell.execute_reply":"2025-05-03T13:38:53.353079Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class MRIDataset(Dataset):\n    def __init__(self, subject_paths, subject_sexes, transform=None):\n        self.subject_paths = subject_paths\n        self.subject_sexes = subject_sexes\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.subject_paths)\n\n    def __getitem__(self, idx):\n        nii_path = self.subject_paths[idx]\n        try:\n            img = nib.load(nii_path).get_fdata()\n            img = np.nan_to_num(img)\n            img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-8)  # tránh chia 0\n            img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n\n            label = 0 if self.subject_sexes[idx] == 'f' else 1\n            label_tensor = torch.tensor(label, dtype=torch.long)\n\n            return img_tensor, label_tensor\n\n        except Exception as e:\n            print(f\"[Warning] Failed to load: {nii_path}\\nError: {e}\")\n            # Cách 1: raise để DataLoader bỏ mẫu này nếu dùng custom sampler\n            raise RuntimeError(f\"Corrupted sample at index {idx}: {nii_path}\")\n\n    # def resize_image(self, img):\n    #     target_shape = (128, 128, 128)\n    #     resized_img = np.resize(img, target_shape)\n    #     return resized_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:39:00.206782Z","iopub.execute_input":"2025-05-03T13:39:00.207304Z","iopub.status.idle":"2025-05-03T13:39:00.213102Z","shell.execute_reply.started":"2025-05-03T13:39:00.207283Z","shell.execute_reply":"2025-05-03T13:39:00.212377Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"clients_dataset = []\nfor i in range(5):\n    dataset = MRIDataset(\n        subject_paths=client_data[i]['subject_path'],\n        subject_sexes=client_data[i]['subject_sex']\n    )\n    clients_dataset.append(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:39:02.093776Z","iopub.execute_input":"2025-05-03T13:39:02.094123Z","iopub.status.idle":"2025-05-03T13:39:02.098202Z","shell.execute_reply.started":"2025-05-03T13:39:02.094094Z","shell.execute_reply":"2025-05-03T13:39:02.097331Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"clients_dataloaders = []\n\nfor dataset in clients_dataset:\n    total_len = len(dataset)\n    train_len = int(0.7 * total_len)\n    val_len = int(0.2 * total_len)\n    test_len = total_len - train_len - val_len  \n\n    train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n\n    train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=8, shuffle=False)\n    test_loader = DataLoader(test_set, batch_size=8, shuffle=False)\n\n    clients_dataloaders.append({\n        'train': train_loader,\n        'val': val_loader,\n        'test': test_loader\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:39:04.440556Z","iopub.execute_input":"2025-05-03T13:39:04.441125Z","iopub.status.idle":"2025-05-03T13:39:04.447428Z","shell.execute_reply.started":"2025-05-03T13:39:04.441102Z","shell.execute_reply":"2025-05-03T13:39:04.446848Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super().__init__()\n        self.add_module('norm1', nn.BatchNorm3d(num_input_features))\n        self.add_module('relu1', nn.ReLU(inplace=True))\n        self.add_module(\n            'conv1',\n            nn.Conv3d(num_input_features,\n                      bn_size * growth_rate,\n                      kernel_size=1,\n                      stride=1,\n                      bias=False))\n        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n        self.add_module('relu2', nn.ReLU(inplace=True))\n        self.add_module(\n            'conv2',\n            nn.Conv3d(bn_size * growth_rate,\n                      growth_rate,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1,\n                      bias=False))\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super().forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features,\n                                     p=self.drop_rate,\n                                     training=self.training)\n        return torch.cat([x, new_features], 1)\n\nclass _DenseBlock(nn.Sequential):\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,\n                 drop_rate):\n        super().__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate,\n                                growth_rate, bn_size, drop_rate)\n            self.add_module('denselayer{}'.format(i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n\n    def __init__(self, num_input_features, num_output_features):\n        super().__init__()\n        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module(\n            'conv',\n            nn.Conv3d(num_input_features,\n                      num_output_features,\n                      kernel_size=1,\n                      stride=1,\n                      bias=False))\n        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    \n    \"\"\"\n    Densenet-BC model class\n    \n    Args:\n        growth_rate (int) - how many filters to add each layer (k in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    \"\"\"\n\n    def __init__(self,\n                 n_input_channels=1,\n                 conv1_t_size=7,\n                 conv1_t_stride=1,\n                 no_max_pool=False,\n                 growth_rate=32,\n                 block_config=(6, 12, 24, 16),\n                 num_init_features=64,\n                 bn_size=4,\n                 drop_rate=0,\n                 num_classes=1):\n\n        super().__init__()\n\n        # First convolution\n        self.features = [('conv1',\n                          nn.Conv3d(n_input_channels,\n                                    num_init_features,\n                                    kernel_size=(conv1_t_size, 7, 7),\n                                    stride=(conv1_t_stride, 2, 2),\n                                    padding=(conv1_t_size // 2, 3, 3),\n                                    bias=False)),\n                         ('norm1', nn.BatchNorm3d(num_init_features)),\n                         ('relu1', nn.ReLU(inplace=True))]\n        if not no_max_pool:\n            self.features.append(\n                ('pool1', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)))\n        self.features = nn.Sequential(OrderedDict(self.features))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers,\n                                num_input_features=num_features,\n                                bn_size=bn_size,\n                                growth_rate=growth_rate,\n                                drop_rate=drop_rate)\n            self.features.add_module('denseblock{}'.format(i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=num_features // 2)\n                self.features.add_module('transition{}'.format(i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight,\n                                        mode='fan_out',\n                                        nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool3d(out,\n                                    output_size=(1, 1,\n                                                 1)).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:39:06.534340Z","iopub.execute_input":"2025-05-03T13:39:06.534728Z","iopub.status.idle":"2025-05-03T13:39:06.555733Z","shell.execute_reply.started":"2025-05-03T13:39:06.534704Z","shell.execute_reply":"2025-05-03T13:39:06.554859Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\n\nclass DenseNetModule(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.lr = 0.01\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def train(self, dataloader, num_epochs=1):\n        self.model.train()\n        self.model.to(device)\n\n        initial_weights = torch.nn.utils.parameters_to_vector(self.model.parameters()).detach().cpu().numpy()\n\n        total_loss = 0.0\n        total_samples = 0\n\n        for epoch in range(num_epochs):\n            print(f\"Epoch {epoch+1}:\")\n            epoch_loss = 0.0\n            epoch_accuracy = 0.0\n            for x_batch, y_batch in dataloader:\n                x_batch = x_batch.to(device)\n                y_batch = y_batch.to(device)\n\n                self.optimizer.zero_grad()\n                loss, acc = self.compute_loss_and_accuracy(x_batch, y_batch)  # Sử dụng hàm này\n\n                loss.backward()\n                self.optimizer.step()\n    \n                batch_size = x_batch.size(0)\n                epoch_loss += loss.item() * batch_size\n                epoch_accuracy += acc.item() * batch_size\n                total_samples += batch_size\n    \n            total_loss += epoch_loss\n            print(f\"Train loss: {epoch_loss / total_samples}, Accuracy: {epoch_accuracy / total_samples}\")\n\n        averaged_loss = total_loss / total_samples\n\n        updated_weights = torch.nn.utils.parameters_to_vector(self.model.parameters()).detach().cpu().numpy()\n        \n        # delta_list = []\n        # for key in initial_weights:\n        #     delta = updated_weights[key] - initial_weights[key]\n        #     delta_list.append(delta.view(-1))  \n        \n        # delta_vector = torch.cat(delta_list) \n\n        delta_vector = updated_weights - initial_weights\n\n        return delta_vector, averaged_loss\n\n    @torch.no_grad()\n    def evaluate(self, dataloader):\n        self.model.eval()\n        self.model.to(device)\n    \n        total_correct = 0\n        total_samples = 0\n        total_loss = 0.0\n        total_accuracy = 0.0\n    \n        for x_batch, y_batch in dataloader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n    \n            loss, acc = self.compute_loss_and_accuracy(x_batch, y_batch)\n    \n            total_loss += loss.item() * x_batch.size(0)\n            total_accuracy += acc.item() * x_batch.size(0)\n            total_samples += x_batch.size(0)\n    \n        avg_loss = total_loss / total_samples\n        avg_accuracy = total_accuracy / total_samples\n\n        print(f\"Test loss: {avg_loss:.2f}, test accuracy: {avg_accuracy:.2f}\")\n    \n        return avg_loss, avg_accuracy\n\n    def compute_loss_and_accuracy(self, x, y):\n        logits = self(x)\n        loss = self.criterion(logits, y.float().unsqueeze(1))\n        preds = (torch.sigmoid(logits) > 0.5).float()\n        acc = (preds == y.unsqueeze(1)).float().mean()\n        return loss, acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:16:39.521018Z","iopub.execute_input":"2025-05-03T13:16:39.521585Z","iopub.status.idle":"2025-05-03T13:16:39.533207Z","shell.execute_reply.started":"2025-05-03T13:16:39.521551Z","shell.execute_reply":"2025-05-03T13:16:39.532463Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class Client:\n    def __init__(self, model, data, epochs=1):\n        self.model = model\n        self.data = data  \n        self.lr = 0.01\n        self.epochs = epochs\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr)\n\n    def train(self):\n        update, loss = self.model.train(self.data, num_epochs=self.epochs)\n        num_samples = len(self.data.dataset)\n        return num_samples, update\n\n    def get_weights(self):\n        return torch.nn.utils.parameters_to_vector(self.model.parameters()).detach().cpu().numpy()\n\n    def set_weights(self, new_weights):\n        vector = torch.tensor(new_weights)\n        torch.nn.utils.vector_to_parameters(vector, self.model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:16:46.863406Z","iopub.execute_input":"2025-05-03T13:16:46.864075Z","iopub.status.idle":"2025-05-03T13:16:46.869030Z","shell.execute_reply.started":"2025-05-03T13:16:46.864049Z","shell.execute_reply":"2025-05-03T13:16:46.868423Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class ServerModel:\n    def __init__(self, model):\n        self.model = model\n        # self.rng = model.rng\n\n    @property\n    def size(self):\n        return self.model.optimizer.size()\n\n    @property\n    def cur_model(self):\n        return self.model\n\n    def send_to(self, clients):\n        \"\"\"Copies server model weights to each client\"\"\"\n        weights = self.get_weights()\n        for c in clients:\n            c.set_weights(weights)\n\n    def get_weights(self):\n        return torch.nn.utils.parameters_to_vector(\n            self.model.model.parameters()\n        ).detach().cpu().numpy()\n\n    def set_weights(self, new_weights):\n        vector = torch.tensor(new_weights, dtype=torch.float32)\n        torch.nn.utils.vector_to_parameters(vector, self.model.model.parameters())\n\n    @staticmethod\n    def weighted_average_oracle(points, weights):\n        \"\"\"Computes weighted average of atoms with specified weights\n\n        Args:\n            points: list, whose weighted average we wish to calculate\n                Each element is a list_of_np.ndarray\n            weights: list of weights of the same length as atoms\n        \"\"\"\n        tot_weights = np.sum(weights)\n        weighted_updates = np.zeros_like(points[0])\n\n        for w, p in zip(weights, points):\n            weighted_updates += (w / tot_weights) * p\n\n        return weighted_updates\n\n    def update(self, updates, max_update_norm=None, maxiter=4, \n            fraction_to_discard=0.0, norm_bound=None, \n        ):\n        \"\"\"Updates server model using given client updates.\n\n        Args:\n            updates: list of (num_samples, update), where num_samples is the\n                number of training samples corresponding to the update, and update\n                is a list of variable weights\n            aggregation: Algorithm used for aggregation. Allowed values are:\n                [ 'mean', 'geom_median']\n            max_update_norm: Reject updates larger than this norm,\n            maxiter: maximum number of calls to the Weiszfeld algorithm if using the geometric median\n        \"\"\"\n        if len(updates) == 0:\n            print('No updates obtained. Continuing without update')\n            return 1, False\n\n        def accept_update(u):\n            # Calculate norm of update\n            norm = np.linalg.norm(u[1])\n            return not (np.isinf(norm) or np.isnan(norm))\n                \n        all_updates = updates\n        updates = [u for u in updates if accept_update(u)]\n        \n        if len(updates) < len(all_updates):\n            print('Rejected {} individual updates because of NaN or Inf'.format(len(all_updates) - len(updates)))\n        if len(updates) == 0:\n            print('All individual updates rejected. Continuing without update')\n            return 1, False\n    \n        points = [u[1] for u in updates]  # List of np.ndarray\n        alphas = [u[0] for u in updates]  # List of num_samples\n    \n        weighted_updates, num_comm_rounds, _ = self.geometric_median_update(points, alphas, maxiter=maxiter)\n    \n        update_norm = np.linalg.norm(weighted_updates)\n    \n        if max_update_norm is None or update_norm < max_update_norm:\n            current_weights = self.get_weights()\n            new_weights = current_weights + weighted_updates\n            self.set_weights(new_weights)\n            updated = True\n        else:\n            print(f\"Update norm = {update_norm} is too large. Update rejected\")\n            updated = False\n    \n        return num_comm_rounds, updated\n        \n    @staticmethod\n    def geometric_median_update(points, alphas, maxiter=4, eps=1e-5, verbose=False, ftol=1e-6):\n        \"\"\"Computes geometric median of atoms with weights alphas using Weiszfeld's Algorithm\n        \"\"\"\n        alphas = np.asarray(alphas, dtype=points[0].dtype) / sum(alphas)\n        median = ServerModel.weighted_average_oracle(points, alphas)\n        num_oracle_calls = 1\n\n        # logging\n        obj_val = ServerModel.geometric_median_objective(median, points, alphas)\n        logs = []\n        log_entry = [0, obj_val, 0, 0]\n        logs.append(log_entry)\n        if verbose:\n            print('Starting Weiszfeld algorithm')\n            print(log_entry)\n\n        # start\n        for i in range(maxiter):\n            prev_median, prev_obj_val = median, obj_val\n            weights = np.asarray([alpha / max(eps, ServerModel.l2dist(median, p)) for alpha, p in zip(alphas, points)],\n                                 dtype=alphas.dtype)\n            weights = weights / weights.sum()\n            median = ServerModel.weighted_average_oracle(points, weights)\n            num_oracle_calls += 1\n            obj_val = ServerModel.geometric_median_objective(median, points, alphas)\n            log_entry = [i+1, obj_val,\n                         (prev_obj_val - obj_val)/obj_val,\n                         ServerModel.l2dist(median, prev_median)]\n            logs.append(log_entry)\n            if verbose:\n                print(log_entry)\n            if abs(prev_obj_val - obj_val) < ftol * obj_val:\n                break\n        return median, num_oracle_calls, logs\n\n    @staticmethod\n    def l2dist(p1, p2):\n        \"\"\"L2 distance between p1, p2, each of which is a list of nd-arrays\"\"\"\n        #return np.linalg.norm([np.linalg.norm(x1 - x2) for x1, x2 in zip(p1, p2)])\n        return np.linalg.norm(p1 - p2)\n\n    @staticmethod\n    def geometric_median_objective(median, points, alphas):\n        \"\"\"Compute geometric median objective.\"\"\"\n        return sum([alpha * ServerModel.l2dist(median, p) for alpha, p in zip(alphas, points)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:16:49.400289Z","iopub.execute_input":"2025-05-03T13:16:49.400591Z","iopub.status.idle":"2025-05-03T13:16:49.414640Z","shell.execute_reply.started":"2025-05-03T13:16:49.400566Z","shell.execute_reply":"2025-05-03T13:16:49.413897Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nbase_model = DenseNet(num_init_features=32, growth_rate=16, block_config=(4, 8, 16, 12))\nglobal_weights = copy.deepcopy(base_model.state_dict())\n\nclients = []\nfor i in range(3):\n    model = DenseNet(num_init_features=32, growth_rate=16, block_config=(4, 8, 16, 12))\n    model.load_state_dict(copy.deepcopy(global_weights))\n    model = DenseNetModule(model.to(device))\n    client = Client(model=model, data=clients_dataloaders[i][\"train\"])\n    clients.append(client)\n\nserver_model = DenseNet(num_init_features=32, growth_rate=16, block_config=(4, 8, 16, 12))\nserver_model.load_state_dict(copy.deepcopy(global_weights))\nserver_model = DenseNetModule(server_model.to(device))\nserver = ServerModel(model=server_model)\n\n\nfor round in range(5):  \n    print(f\"Round {round + 1}\")\n    \n    server.send_to(clients)\n\n    updates = []\n    for client_index, client in enumerate(clients):\n        print(f\"Training on clients {client_index + 1}\")\n        num_samples, update = client.train() \n        updates.append((num_samples, update))\n        loss, acc = client.model.evaluate(clients_dataloaders[client_index][\"val\"])\n    print(f\"Update server on round {round + 1}\")\n    server.update(updates)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:39:13.007391Z","iopub.execute_input":"2025-05-03T13:39:13.007995Z","iopub.status.idle":"2025-05-03T15:06:41.338946Z","shell.execute_reply.started":"2025-05-03T13:39:13.007971Z","shell.execute_reply":"2025-05-03T15:06:41.338154Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/186119331.py:125: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n","output_type":"stream"},{"name":"stdout","text":"Round 1\nTraining on clients 1\nEpoch 1:\nTrain loss: 0.6129160450463169, Accuracy: 0.6703755217235178\nTest loss: 1.00, test accuracy: 0.54\nTraining on clients 2\nEpoch 1:\nTrain loss: 0.6161303473778992, Accuracy: 0.634214186535757\nTest loss: 0.86, test accuracy: 0.52\nTraining on clients 3\nEpoch 1:\nTrain loss: 0.6228067395750106, Accuracy: 0.6411682894564802\nTest loss: 0.67, test accuracy: 0.59\nUpdate server on round 1\nRound 2\nTraining on clients 1\nEpoch 1:\nTrain loss: 0.5359035450991072, Accuracy: 0.7593880391087751\nTest loss: 0.71, test accuracy: 0.50\nTraining on clients 2\nEpoch 1:\nTrain loss: 0.5216137270270866, Accuracy: 0.7482614742698191\nTest loss: 0.70, test accuracy: 0.64\nTraining on clients 3\nEpoch 1:\nTrain loss: 0.5282685822173849, Accuracy: 0.7538247566063978\nTest loss: 0.45, test accuracy: 0.81\nUpdate server on round 2\nRound 3\nTraining on clients 1\nEpoch 1:\nTrain loss: 0.45896652316019165, Accuracy: 0.7934631434203189\nTest loss: 0.70, test accuracy: 0.60\nTraining on clients 2\nEpoch 1:\nTrain loss: 0.4972418407820196, Accuracy: 0.7628650904033379\nTest loss: 0.67, test accuracy: 0.54\nTraining on clients 3\nEpoch 1:\nTrain loss: 0.48391806034783164, Accuracy: 0.7816411684550895\nTest loss: 0.60, test accuracy: 0.64\nUpdate server on round 3\nRound 4\nTraining on clients 1\nEpoch 1:\nTrain loss: 0.4532254765196205, Accuracy: 0.80876216984591\nTest loss: 0.60, test accuracy: 0.74\nTraining on clients 2\nEpoch 1:\nTrain loss: 0.4601818049981962, Accuracy: 0.7865090404995957\nTest loss: 0.43, test accuracy: 0.81\nTraining on clients 3\nEpoch 1:\nTrain loss: 0.4215223433991301, Accuracy: 0.8115438110141993\nTest loss: 0.43, test accuracy: 0.79\nUpdate server on round 4\nRound 5\nTraining on clients 1\nEpoch 1:\nTrain loss: 0.44004429609453893, Accuracy: 0.8164116829758061\nTest loss: 0.67, test accuracy: 0.57\nTraining on clients 2\nEpoch 1:\nTrain loss: 0.4059062706246993, Accuracy: 0.8219749653123848\nTest loss: 0.36, test accuracy: 0.84\nTraining on clients 3\nEpoch 1:\nTrain loss: 0.43092878467516044, Accuracy: 0.8018080669251868\nTest loss: 0.39, test accuracy: 0.83\nUpdate server on round 5\n","output_type":"stream"}],"execution_count":38}]}

