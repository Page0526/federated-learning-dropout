{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11680004,"sourceType":"datasetVersion","datasetId":7180861},{"sourceId":11721921,"sourceType":"datasetVersion","datasetId":7270261}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q lightning flwr wandb hydra-core","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:06.540985Z","iopub.execute_input":"2025-05-21T18:21:06.541349Z","iopub.status.idle":"2025-05-21T18:21:11.059341Z","shell.execute_reply.started":"2025-05-21T18:21:06.541320Z","shell.execute_reply":"2025-05-21T18:21:11.058586Z"}},"outputs":[],"execution_count":220},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom pathlib import Path\nimport nibabel as nib\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom typing import Dict, List, Optional, Tuple, Union","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.060799Z","iopub.execute_input":"2025-05-21T18:21:11.061098Z","iopub.status.idle":"2025-05-21T18:21:11.066499Z","shell.execute_reply.started":"2025-05-21T18:21:11.061073Z","shell.execute_reply":"2025-05-21T18:21:11.065915Z"}},"outputs":[],"execution_count":221},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb_api_key\"\nWANDB_APIKEY = UserSecretsClient().get_secret(secret_label)\n\nROOT_PATH = '/kaggle/input/mini-brain3d-dataset/not_skull_stripped'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.067234Z","iopub.execute_input":"2025-05-21T18:21:11.067488Z","iopub.status.idle":"2025-05-21T18:21:11.210194Z","shell.execute_reply.started":"2025-05-21T18:21:11.067465Z","shell.execute_reply":"2025-05-21T18:21:11.209671Z"}},"outputs":[],"execution_count":222},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class MRIDataset(Dataset) :\n\n    def __init__(self, root_dir: str, label_path: str = None, transform = None, label_df: pd.DataFrame = None, is_3d: bool = False):\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.is_3d = is_3d\n        if label_df is None:\n          self.labels_df = pd.read_csv(label_path)\n          \n        else :\n          self.labels_df = label_df\n\n        self.labels_df['subject_id'] = self.labels_df['subject_id'].astype(str)\n        self.labels_df = self.labels_df[self.labels_df['subject_dx'] == 'control']\n\n        all_nii_files = list(self.root_dir.rglob(\"*.nii\"))\n        fail_paths = [\"sub-BrainAge005600/anat/sub-BrainAge005600_T1w.nii/sub-BrainAge005600_T1w.nii\"]\n        self.file_paths = [fp for fp in all_nii_files if fp.is_file() and fp.name not in fail_paths ]\n\n        valid_subjects = set(self.labels_df['subject_id'].values)\n\n        self.file_paths = [fp for fp in self.file_paths if any(vs in str(fp) for vs in valid_subjects)]\n        self.file_paths.sort()\n\n\n\n    def __len__(self):\n        return len(self.file_paths)\n\n\n    def preprocessing_datapoint(self, img_data):\n\n        mid_x = img_data.shape[0] // 2\n        mid_y = img_data.shape[1] // 2\n        mid_z = img_data.shape[2] // 2\n\n        axial_slice = img_data[:, :, mid_z]\n        coronal_slice = img_data[:, mid_y, :]\n        sagittal_slice = img_data[mid_x, :, :]\n\n\n        combined_data = np.stack([axial_slice, coronal_slice, sagittal_slice], axis=0)\n        combined_data = torch.from_numpy(combined_data).float()\n\n        if self.transform : combined_data = self.transform(combined_data)\n\n        return combined_data\n\n\n\n\n    def __getitem__(self, idx):\n        img_path = self.file_paths[idx]\n        file_path_str = str(img_path)\n\n        subject_id = None\n        valid_subjects_set = set(self.labels_df['subject_id'].values)\n\n\n        for sid in valid_subjects_set:\n            if sid in file_path_str:\n                subject_id = sid\n                break\n\n        if subject_id is None:\n            raise ValueError(f\"Không tìm thấy subject_id cho file: {img_path}\")\n\n        metadata = self.labels_df.loc[self.labels_df['subject_id'] == subject_id].iloc[0].to_dict()\n\n        img_data = nib.load(img_path).get_fdata()\n\n        img_data = torch.from_numpy(img_data).unsqueeze(0).float()\n\n        label = 0\n        if metadata['subject_sex'] == 'm' : label = 1\n\n        if not self.is_3d:\n            img_data = self.preprocessing_datapoint(img_data)\n\n        return img_data,  label\n\n\n\ndef visualize_sample(dataset, idx):\n    mri_data, label = dataset[idx]\n    title = f\"Label: {label}\\n\"\n    plt.close('all')\n    fig = plt.figure(figsize = (18, 6))\n\n    if isinstance(mri_data, torch.Tensor):\n        data = mri_data.squeeze().numpy()\n    else:\n        data = mri_data\n\n\n    ax1 = fig.add_subplot(1, 3, 1)\n    plt.imshow(data[0, :, :].T, cmap='gray', origin='lower')\n\n    ax2 = fig.add_subplot(1, 3, 2)\n    ax2.imshow(data[1, :, :].T, cmap='gray', origin='lower')\n\n    ax3 = fig.add_subplot(1, 3, 3)\n    ax3.imshow(data[2, :, :].T, cmap='gray', origin='lower')\n\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.211939Z","iopub.execute_input":"2025-05-21T18:21:11.212206Z","iopub.status.idle":"2025-05-21T18:21:11.228134Z","shell.execute_reply.started":"2025-05-21T18:21:11.212190Z","shell.execute_reply":"2025-05-21T18:21:11.227552Z"}},"outputs":[],"execution_count":223},{"cell_type":"markdown","source":"## Data splitting","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split \n\ndef preprocessing_labels(df: pd.DataFrame, root_dir: str = ROOT_PATH):\n    subject_list = []\n    for root, dirs, files in os.walk(root_dir):\n      for dir_name in dirs:\n        if dir_name.startswith(\"sub-BrainAge\"):\n            subject_list.append(dir_name)\n\n    return df[df['subject_id'].isin(subject_list)]\n\n\ndef prepare_data(data: pd.DataFrame):\n    df = data.copy()\n    df['age_group'] = pd.qcut(df['subject_age'], q = min(5, len(df)), labels = False)\n    df['key'] = df.apply(lambda row : f\"{row['age_group']}_{row['subject_sex']}\", axis = 1)\n    return df\n\n\ndef sampling_data(data, size, random_state ):\n  samples = data.groupby('key', group_keys = False)\n  samples = samples.apply(lambda x: x.sample(\n      n = min(int(size / len(data['key'].unique())), len(x)),\n      replace = len(x) < int(size / len(data['key'].unique())),\n      random_state =  random_state\n  ), include_groups=False)\n\n\n  if len(samples) < size:\n    additional_samples = data.drop(samples.index).sample(\n        n = min(size - len(samples), len(data) - len(samples)),\n        replace = True,\n        random_state = random_state\n    )\n\n    samples = pd.concat([samples, additional_samples])\n  return samples\n\n\ndef create_train_test(sample_labels: list, val_ratio: float = 0.2, root_dir: str = ROOT_PATH, is_3d: bool = False):\n    \n  client_datasets = []\n  for label_df in sample_labels:\n    dataset = MRIDataset(root_dir=root_dir, label_df = label_df, is_3d = is_3d)\n    \n    train_dataset, val_dataset = random_split(dataset, [1 - val_ratio, val_ratio])\n    client_datasets.append((train_dataset, val_dataset))\n  return client_datasets\n    \n\ndef distributed_data_to_clients(data: pd.DataFrame, num_clients: int, overlap_ratio: float):\n\n  df = prepare_data(data)\n\n  n_samples = len(df)\n  samples_per_client = int(n_samples / (num_clients * (1 - overlap_ratio) + overlap_ratio))\n\n  client_datasets = []\n  selected_samples = {}\n\n  # Tạo các client datasets với sự phân bố cân bằng\n  for client_idx in range(num_clients):\n\n      if client_idx == 0:\n          client_data = df.sample(n=samples_per_client, random_state=42+client_idx)\n      else:\n          # overlap size\n          overlap_size = int(samples_per_client * overlap_ratio)\n          non_overlap_size = samples_per_client - overlap_size\n\n          # building overlap\n          all_previous_samples = pd.DataFrame()\n          for prev_client_idx in range(client_idx):\n              all_previous_samples = pd.concat([all_previous_samples, selected_samples[prev_client_idx]])\n\n          # sampling\n          if len(all_previous_samples) > 0:\n              overlap_samples = sampling_data(all_previous_samples, overlap_size, client_idx * 100 + 42)\n          else:\n              overlap_samples = pd.DataFrame(columns=df.columns)\n\n          # Lấy mẫu mới (không overlap)\n          remaining_indices = df.index.difference(all_previous_samples.index)\n          if len(remaining_indices) > 0:\n              remaining_df = df.loc[remaining_indices]\n              non_overlap_samples = sampling_data(remaining_df, non_overlap_size, client_idx * 100 + 42)\n          else:\n\n              non_overlap_samples = df.sample(n=non_overlap_size, replace=True, random_state=42+client_idx*300)\n\n\n          client_data = pd.concat([overlap_samples, non_overlap_samples])\n\n\n      selected_samples[client_idx] = client_data\n      client_datasets.append(client_data.drop(['age_group', 'key'], axis=1))\n\n  return client_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.229049Z","iopub.execute_input":"2025-05-21T18:21:11.229555Z","iopub.status.idle":"2025-05-21T18:21:11.249614Z","shell.execute_reply.started":"2025-05-21T18:21:11.229532Z","shell.execute_reply":"2025-05-21T18:21:11.249072Z"}},"outputs":[],"execution_count":224},{"cell_type":"code","source":"def iid_client_split(dataset, num_client = 3,  val_ratio = 0.2):\n\n    client_datasets = []\n    sample_per_client = len(dataset) // num_client\n\n\n    for i in range(num_client):\n        start_idx = i * sample_per_client\n        end_idx = (i + 1) * sample_per_client if i < num_client - 1 else len(dataset)\n        indecies = list(range(start_idx, end_idx))\n\n        client_dataset = torch.utils.data.Subset(dataset, indecies)\n        train_dataset, val_dataset = random_split(client_dataset, [1 - val_ratio, val_ratio])\n\n        client_datasets.append((train_dataset, val_dataset))\n    return client_datasets\n\n\ndef same_distribution_client_split(dataset, num_client, val_ratio = 0.2, overlap_ratio = 0.2, root_dir = ROOT_PATH, is_3d = False):\n    \"\"\"\n    Split the dataset into clients with the same distribution of labels.\n    \"\"\"\n    labels_df = dataset.labels_df\n    labels_df = preprocessing_labels(labels_df, root_dir = root_dir)    \n    labels_df = prepare_data(labels_df)\n\n    client_datasets = distributed_data_to_clients(labels_df, num_clients=num_client, overlap_ratio=overlap_ratio)\n\n    client_datasets = create_train_test(client_datasets, val_ratio=val_ratio, root_dir=root_dir, is_3d = is_3d)\n\n    return client_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.250288Z","iopub.execute_input":"2025-05-21T18:21:11.250538Z","iopub.status.idle":"2025-05-21T18:21:11.269987Z","shell.execute_reply.started":"2025-05-21T18:21:11.250520Z","shell.execute_reply":"2025-05-21T18:21:11.269331Z"}},"outputs":[],"execution_count":225},{"cell_type":"markdown","source":"#  Model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom torchsummary import summary\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super().__init__()\n        self.add_module('norm1', nn.BatchNorm3d(num_input_features))\n        self.add_module('relu1', nn.ReLU(inplace=True))\n        self.add_module(\n            'conv1',\n            nn.Conv3d(num_input_features,\n                      bn_size * growth_rate,\n                      kernel_size=1,\n                      stride=1,\n                      bias=False))\n        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n        self.add_module('relu2', nn.ReLU(inplace=True))\n        self.add_module(\n            'conv2',\n            nn.Conv3d(bn_size * growth_rate,\n                      growth_rate,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1,\n                      bias=False))\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super().forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features,\n                                     p=self.drop_rate,\n                                     training=self.training)\n        return torch.cat([x, new_features], 1)\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super().__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate,\n                                growth_rate, bn_size, drop_rate)\n            self.add_module('denselayer{}'.format(i + 1), layer)\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super().__init__()\n        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module(\n            'conv',\n            nn.Conv3d(num_input_features,\n                      num_output_features,\n                      kernel_size=1,\n                      stride=1,\n                      bias=False))\n        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n\nclass DenseNet(nn.Module):\n    def __init__(self,\n                 n_input_channels=1,\n                 conv1_t_size=7,\n                 conv1_t_stride=1,\n                 no_max_pool=False,\n                 growth_rate=16,\n                 block_config=(4, 8, 16, 12),\n                 num_init_features=32,\n                 bn_size=4,\n                 drop_rate=0,\n                 num_classes=2):\n        super().__init__()\n\n        # First convolution\n        self.features = [('conv1',\n                          nn.Conv3d(n_input_channels,\n                                    num_init_features,\n                                    kernel_size=(conv1_t_size, 7, 7),\n                                    stride=(conv1_t_stride, 2, 2),\n                                    padding=(conv1_t_size // 2, 3, 3),\n                                    bias=False)),\n                         ('norm1', nn.BatchNorm3d(num_init_features)),\n                         ('relu1', nn.ReLU(inplace=True))]\n        if not no_max_pool:\n            self.features.append(\n                ('pool1', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)))\n        self.features = nn.Sequential(OrderedDict(self.features))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers,\n                                num_input_features=num_features,\n                                bn_size=bn_size,\n                                growth_rate=growth_rate,\n                                drop_rate=drop_rate)\n            self.features.add_module('denseblock{}'.format(i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=num_features // 2)\n                self.features.add_module('transition{}'.format(i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Khởi tạo trọng số\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool3d(out, output_size=(1, 1, 1)).view(features.size(0), -1)\n        logits = self.classifier(out)\n        return out, logits\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.270793Z","iopub.execute_input":"2025-05-21T18:21:11.271080Z","iopub.status.idle":"2025-05-21T18:21:11.291601Z","shell.execute_reply.started":"2025-05-21T18:21:11.271063Z","shell.execute_reply":"2025-05-21T18:21:11.290996Z"}},"outputs":[],"execution_count":226},{"cell_type":"markdown","source":"## Relaxed Balanced Softmax & feature augmentation","metadata":{}},{"cell_type":"code","source":"class RelaxedBSM(nn.Module):\n    def __init__(self, dataloader, num_classes, eps=0.01, device=None):\n        super(RelaxedBSM, self).__init__()\n        self.num_classes = num_classes\n        self.dataloader = dataloader\n        self.eps = eps\n        \n        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n        self._cached_prior = None\n        self.to(self.device)\n\n    def prior_y(self, recalculate=False):\n        if self._cached_prior is not None and not recalculate:\n            return self._cached_prior\n            \n        py = torch.zeros(self.num_classes, device=self.device)\n        total_samples = 0\n        with torch.no_grad():\n            for _, labels in self.dataloader:\n                labels = labels.to(self.device)\n\n                if not labels.dtype.is_floating_point:\n                    bin_counts = torch.bincount(labels, minlength=self.num_classes).float()\n                else:\n                    bin_counts = labels.sum(dim=0) if labels.dim() > 1 \\\n                    else torch.zeros(self.num_classes, device=self.device).scatter_add_(0, labels.long(), torch.ones_like(labels, device=self.device))\n\n                py += bin_counts\n                total_samples += labels.size(0)\n        self._cached_prior = py / total_samples if total_samples > 0 else torch.ones(self.num_classes, device=self.device) / self.num_classes\n        return self._cached_prior\n\n    def smooth_distribution(self, py):\n        \"\"\"Apply smoothing to a probability distribution\"\"\"\n        py_smooth = (1 - self.eps) * py + self.eps / self.num_classes\n        return py_smooth / py_smooth.sum()\n\n    def prior_y_batch(self, labels):\n        labels = labels.to(self.device)\n        py = torch.bincount(labels, minlength=self.num_classes).float()\n        return py / labels.size(0)\n\n    def bsm1(self, logit):\n        py = self.prior_y()\n        py_smooth = self.smooth_distribution(py)\n\n        logit = logit - logit.max(dim=1, keepdim=True)[0]\n        exp_logits = torch.exp(logit)\n        \n        pc_exp = exp_logits * py_smooth.unsqueeze(0)\n        denominator = pc_exp.sum(dim=1, keepdim=True) + 1e-8\n        \n        return pc_exp / denominator\n\n    def bsm2(self, logit, py):\n        \"\"\"Balanced Softmax with provided class distribution\"\"\"\n        py = py.to(self.device)\n        py_smooth = self.smooth_distribution(py)\n        \n        logit = logit - logit.max(dim=1, keepdim=True)[0]\n        exp_logits = torch.exp(logit)\n        \n        pc_exp = exp_logits * py_smooth.unsqueeze(0)\n        denominator = pc_exp.sum(dim=1, keepdim=True) + 1e-8\n        \n        return pc_exp / denominato\n    \n    def forward(self, logit, py=None):\n        \"\"\"Forward method for nn.Module compatibility\"\"\"\n        if py is None:\n            return self.bsm1(logit)\n        else:\n            return self.bsm2(logit, py)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.292259Z","iopub.execute_input":"2025-05-21T18:21:11.292422Z","iopub.status.idle":"2025-05-21T18:21:11.309904Z","shell.execute_reply.started":"2025-05-21T18:21:11.292410Z","shell.execute_reply":"2025-05-21T18:21:11.309402Z"}},"outputs":[],"execution_count":227},{"cell_type":"code","source":"def get_prototypes(model, dataloader, num_classes, device=None):\n    model.eval()\n    prototypes = [[] for _ in range(num_classes)]\n    with torch.no_grad():\n        for im, labels in dataloader:\n            im = im.to(device, non_blocking = True)\n            labels = labels.to(device, non_blocking = True)\n            model = model.to(device)\n            features, _ = model(im)\n            for i, label in enumerate(labels):\n                prototypes[label.item()].append(features[i])\n                \n    class_prototypes = torch.zeros((num_classes, features.shape[1]), device = device)\n    for c in range(num_classes):\n        if prototypes[c]:\n            # average over all feature vectors from the same class\n            class_prototypes[c] = torch.stack(prototypes[c]).mean(dim = 0)\n        else:\n            class_prototypes[c] = torch.zeros(features.shape[1], device = device) # pseudo-prototypes for missing class in dataloader\n            \n    return class_prototypes # [num_classes, feature_vectors]\n\ndef feature_augmentation(features, labels, prototypes, num_classes, lam=1.0, device=None):\n    \"\"\"Perform feature augmentation by transferring intra-class variance to missing classes.\"\"\"\n    aug_features = []\n    aug_labels = []\n    for i in range(len(labels)):\n        src_class = labels[i].item()\n        # Cycle through classes for augmentation\n        tgt_class = (src_class + 1) % num_classes\n        if torch.all(prototypes[tgt_class] == 0):\n            continue\n        # Compute augmented feature: \\tilde{h}_{j,k} = p_j + \\lambda (h_{i,k} - p_i)\n        aug_feature = prototypes[tgt_class].to(device) + lam * (features[i] - prototypes[src_class].to(device))\n        aug_features.append(aug_feature)\n        aug_labels.append(tgt_class)\n    \n    if aug_features:\n        aug_features = torch.stack(aug_features)\n        aug_labels = torch.tensor(aug_labels, dtype=torch.long, device=features.device)\n        return aug_features, aug_labels\n    else:\n        return None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.310685Z","iopub.execute_input":"2025-05-21T18:21:11.310903Z","iopub.status.idle":"2025-05-21T18:21:11.329367Z","shell.execute_reply.started":"2025-05-21T18:21:11.310888Z","shell.execute_reply":"2025-05-21T18:21:11.328780Z"}},"outputs":[],"execution_count":228},{"cell_type":"markdown","source":"## Lightning module","metadata":{}},{"cell_type":"code","source":"import lightning as pl\nfrom torchmetrics import Accuracy, F1Score, Precision, Recall, MeanMetric\nimport torch.optim as optim \n\n\nclass DenseNetModule(pl.LightningModule):\n    def __init__(self, net, rbsm=None, global_prototypes=None, learning_rate=1e-3, weight_decay = 1e-2, batch_size = 32, lam=1, mu=0.1):\n        super().__init__()\n        self.model = net\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.batch_size = batch_size\n        if global_prototypes:\n            self.global_prototypes = global_prototypes\n        else:\n            self.global_prototypes = [torch.zeros(364) for _ in range(2)]\n        self.lam = lam\n        self.mu = mu\n        \n        # how confidence model is in it prediction\n        # tức model có thể rất tự tin trong quyết định nhưng thực tế lại sai\n        # BCE = y*log(y_pred) + (1 - y)*log(1 - y_pred)\n        self.criterion = nn.CrossEntropyLoss()\n        self.nll_loss = nn.NLLLoss()\n        self.rbsm = rbsm\n        \n        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        features, logits = self(x)\n        \n        balanced_probs = self.rbsm(logits)\n        loglikelihood = torch.log(balanced_probs + 1e-8)\n        loss0 = self.nll_loss(loglikelihood, y)\n        loss1 = 0.0\n        aug_features, aug_labels = feature_augmentation(features, y, self.global_prototypes, 2, self.lam, self.device)\n        if aug_features is not None: \n            aug_logits = self.model.classifier(aug_features)\n            aug_probs = self.rbsm(aug_logits)\n            aug_loglikelihood = torch.log(aug_probs + 1e-8)\n            loss1 = self.nll_loss(aug_loglikelihood, aug_labels)\n\n        loss = loss0 + self.mu * loss1\n        acc = self.train_accuracy(torch.argmax(logits, dim=1), y)\n\n        self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        _, logits = self(x)\n        \n        loss = self.criterion(logits, y)\n        acc = self.val_accuracy(torch.argmax(logits, dim=1), y)\n        \n        self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        return loss\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        _, logits = self(x)\n        \n        loss = self.criterion(logits, y)\n        acc = self.test_accuracy(torch.argmax(logits, dim=1), y)\n\n        self.log('test/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('test/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n            \n    def configure_optimizers(self):\n        optimizer =  torch.optim.SGD(self.parameters(), lr=self.learning_rate, weight_decay = self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n\n        return {\n           \"optimizer\": optimizer,\n           \"lr_scheduler\": {\n               \"scheduler\": scheduler,\n               \"monitor\": \"val_loss\",\n           },\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.331903Z","iopub.execute_input":"2025-05-21T18:21:11.332121Z","iopub.status.idle":"2025-05-21T18:21:11.348385Z","shell.execute_reply.started":"2025-05-21T18:21:11.332106Z","shell.execute_reply":"2025-05-21T18:21:11.347649Z"}},"outputs":[],"execution_count":229},{"cell_type":"markdown","source":"# Client","metadata":{}},{"cell_type":"code","source":"from collections import OrderedDict\nfrom flwr.client import NumPyClient\nfrom flwr.common import  Context\nfrom torch.utils.data import DataLoader\nimport logging \nimport warnings \nfrom lightning.pytorch.loggers.wandb import WandbLogger\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.349102Z","iopub.execute_input":"2025-05-21T18:21:11.349274Z","iopub.status.idle":"2025-05-21T18:21:11.366710Z","shell.execute_reply.started":"2025-05-21T18:21:11.349261Z","shell.execute_reply":"2025-05-21T18:21:11.366199Z"}},"outputs":[],"execution_count":230},{"cell_type":"code","source":"class FlowerLightningClient(NumPyClient):\n\n    def __init__(self, model: pl.LightningModule, train_dataloader, val_dataloader, epochs, batch_size, device, client_id): \n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        self.epochs = epochs\n        self.device = device \n        self.client_id = client_id\n        self.model = model\n        self.batch_size = batch_size\n\n\n    def get_parameters(self, config):\n        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n\n    def set_parameters(self, parameters):\n        if not parameters:\n            return\n    \n        params_dict = zip(self.model.state_dict().keys(), parameters)\n        state_dict = OrderedDict((k, torch.tensor(v)) for k, v in params_dict)\n    \n        if state_dict:\n            self.model.load_state_dict(state_dict, strict=False)\n\n        self.model = self.model.to(self.device)\n\n        \n    def fit(self, parameters, config):\n\n        self.set_parameters(parameters)\n        \n        checkpoint_callback = ModelCheckpoint(\n            dirpath=f\"./checkpoints/client_{self.client_id}\",\n            filename=f\"round_{config.get('round_num', 0)}\" + \"-{epoch:02d}\",\n            save_top_k=1,\n            monitor=\"val/loss\",\n            mode=\"min\"\n        )\n\n        trainer = pl.Trainer(\n            max_epochs=self.epochs,\n            accelerator=\"auto\",\n            devices=1,\n            callbacks=[checkpoint_callback],\n            enable_progress_bar=False, \n            log_every_n_steps=1\n        )\n        \n        global_prototypes = pickle.loads(config['global_prototypes'])\n        self.model.global_prototypes = global_prototypes\n        rbsm = RelaxedBSM(self.train_dataloader, num_classes=2, device=self.device)\n        self.model.rbsm = rbsm.to(self.device)\n        self.model.model = self.model.model.to(self.device)\n        trainer.fit(self.model.to(self.device), train_dataloaders=self.train_dataloader, val_dataloaders=self.val_dataloader)\n        local_prototypes = get_prototypes(self.model, self.train_dataloader, 2, self.device)\n        \n        callback_metrics = trainer.callback_metrics\n\n        train_loss = callback_metrics.get(\"train/loss\", 0)\n        train_accuracy = callback_metrics.get(\"train/acc\", 0)\n        val_loss = callback_metrics.get(\"val/loss\", 0)\n        val_accuracy = callback_metrics.get(\"val/acc\", 0)\n    \n        # Calculate label counts\n        from collections import Counter\n        label_counts = Counter()\n        for _, label in self.train_dataloader.dataset:\n            if isinstance(label, torch.Tensor):\n                label = label.item()\n            label_counts[label] += 1\n            \n        metrics = {\n            \"train_loss\": train_loss.item() if isinstance(train_loss, torch.Tensor) else float(train_loss),\n            \"train_accuracy\": train_accuracy.item() if isinstance(train_accuracy, torch.Tensor) else float(train_accuracy),\n            \"val_loss\": val_loss.item() if isinstance(val_loss, torch.Tensor) else float(val_loss),\n            \"val_accuracy\": val_accuracy.item() if isinstance(val_accuracy, torch.Tensor) else float(val_accuracy),\n            \"local_prototypes\": pickle.dumps(local_prototypes),\n            \"label_counts\": pickle.dumps(label_counts)\n        }\n\n        return self.get_parameters(config={}), len(self.train_dataloader.dataset), metrics\n\n\n    def evaluate(self, parameters, config):\n\n        self.set_parameters(parameters)\n\n        trainer = pl.Trainer(\n            accelerator=\"auto\",\n            devices=1 ,\n            enable_progress_bar=False\n        )\n\n        results = trainer.test(self.model, dataloaders=self.val_dataloader)\n        \n        callback_metrics = trainer.callback_metrics\n\n        test_loss = callback_metrics.get(\"test/loss\")\n        test_accuracy = callback_metrics.get(\"test/acc\")\n        \n        # Additional metrics\n        metrics = {\n            \"test_loss\": test_loss.item() if isinstance(test_loss, torch.Tensor) else float(test_loss),\n            \"test_accuracy\": test_accuracy.item() if isinstance(test_accuracy, torch.Tensor) else float(test_accuracy),\n        }\n        return float(test_loss), len(self.val_dataloader.dataset), metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.367388Z","iopub.execute_input":"2025-05-21T18:21:11.367660Z","iopub.status.idle":"2025-05-21T18:21:11.383351Z","shell.execute_reply.started":"2025-05-21T18:21:11.367637Z","shell.execute_reply":"2025-05-21T18:21:11.382590Z"}},"outputs":[],"execution_count":231},{"cell_type":"code","source":"def create_lightning_client_fn(device, epochs, client_datasets, batch_size, num_workers, pl_model):\n\n    def client_fn(context: Context):\n        \n        client_id = context.node_config['partition-id']\n        train_dataset, val_dataset = client_datasets[client_id]\n\n        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n        val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers = num_workers)\n\n        return FlowerLightningClient(pl_model, train_dataloader, val_dataloader, epochs, batch_size, device, client_id).to_client()\n\n    return client_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.383999Z","iopub.execute_input":"2025-05-21T18:21:11.384199Z","iopub.status.idle":"2025-05-21T18:21:11.401091Z","shell.execute_reply.started":"2025-05-21T18:21:11.384184Z","shell.execute_reply":"2025-05-21T18:21:11.400431Z"}},"outputs":[],"execution_count":232},{"cell_type":"code","source":"import random\nimport flwr as fl\nfrom flwr.common import (\n    EvaluateRes,\n    FitIns,\n    FitRes,\n    Parameters,\n    EvaluateIns,\n)\nfrom flwr.server.client_proxy import ClientProxy\nfrom flwr.server.client_manager import ClientManager\nfrom flwr.server.strategy import FedAvg\nfrom flwr.common.parameter import parameters_to_ndarrays","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.401709Z","iopub.execute_input":"2025-05-21T18:21:11.401915Z","iopub.status.idle":"2025-05-21T18:21:11.420172Z","shell.execute_reply.started":"2025-05-21T18:21:11.401900Z","shell.execute_reply":"2025-05-21T18:21:11.419643Z"}},"outputs":[],"execution_count":233},{"cell_type":"code","source":"from flwr.common import ndarrays_to_parameters\n\ndef get_parameters(model) -> List[np.ndarray]:\n    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n    \nclass DropoutFedAvg(FedAvg):\n    \"\"\"FedAvg strategy with client dropout simulation and metrics tracking.\"\"\"\n\n    def __init__( self, net, dropout_rate_training: float = 0.3, dropout_rate_eval: float = 0.3, fixed_clients: Optional[List[int]] = None, dropout_pattern_train: str = \"random\", dropout_pattern_eval: str = \"random\", **kwargs):\n    \n        if \"fit_metrics_aggregation_fn\" not in kwargs:\n            kwargs[\"fit_metrics_aggregation_fn\"] = self.weighted_average\n        if \"evaluate_metrics_aggregation_fn\" not in kwargs:\n            kwargs[\"evaluate_metrics_aggregation_fn\"] = self.weighted_average\n\n        super().__init__(**kwargs)\n        self.dropout_rate_training = dropout_rate_training\n        self.dropout_rate_eval = dropout_rate_eval\n        self.fixed_clients = fixed_clients or []\n        self.dropout_pattern_train = dropout_pattern_train\n        self.dropout_pattern_eval = dropout_pattern_eval\n        self.current_round = 0\n        self.dropped_clients_history_training: Dict[int, List[int]] = {}\n        self.dropped_clients_history_evaluation: Dict[int, List[int]] = {}\n\n        # For tracking metrics\n        self.fit_metrics_history: List[Dict[str, float]] = []\n        self.eval_metrics_history: List[Dict[str, float]] = []\n\n        self.net = net\n        self.global_prototypes = [torch.zeros(364) for _ in range(2)] # create pseudo-global prototypes\n    \n    def weighted_average(self, metrics: List[Tuple[int, Dict]]) -> Dict:\n        \"\"\"Aggregate metrics using weighted average based on number of samples.\"\"\"\n        if not metrics:\n            return {}\n\n        total_examples = sum([num_examples for num_examples, _ in metrics])\n        weighted_metrics = {}\n\n        for metric_key in [\"train_loss\", \"train_accuracy\", \"val_loss\", \"val_accuracy\", \"test_accuracy\", \"test_loss\"]:\n            values = [\n                metric_dict[metric_key] * num_examples\n                for num_examples, metric_dict in metrics\n                if metric_key in metric_dict and isinstance(metric_dict[metric_key], (int, float))\n            ]\n            if values:\n                weighted_sum = sum(values)\n                weighted_metrics[metric_key] = weighted_sum / total_examples if total_examples > 0 else 0\n\n        return weighted_metrics\n\n\n    def configure_fit( self, server_round: int, parameters: Parameters, client_manager: ClientManager) -> List[Tuple[ClientProxy, FitIns]]:\n        \"\"\"Configure the next round of training with client dropout.\"\"\"\n        self.current_round = server_round\n\n    \n        client_fit_instructions = super().configure_fit(\n            server_round, parameters, client_manager, \n        )\n\n        if not client_fit_instructions:\n            return []\n\n\n        available_clients = self._apply_dropout(client_fit_instructions, dropout_rate=self.dropout_rate_training, dropout_pattern=self.dropout_pattern_train)\n\n        # Save dropout history for this round\n        client_ids = [int(client.cid) for client, _ in client_fit_instructions]\n        available_client_ids = [int(client.cid) for client, _ in available_clients]\n        dropped_clients = [cid for cid in client_ids if cid not in available_client_ids]\n        self.dropped_clients_history_training[server_round] = dropped_clients\n\n        print(f\"Round {server_round}: {len(dropped_clients)} clients dropped out of {len(client_ids)} during training\")\n        print(f\"Dropped client IDs: {dropped_clients}\")\n\n        # send global prototypes to clients\n        updated_clients = []\n        for client, fit_ins in available_clients:\n            custom_config = dict(fit_ins.config)\n            custom_config['global_prototypes'] = pickle.dumps(self.global_prototypes)\n            new_fit_ins = FitIns(parameters=fit_ins.parameters, config=custom_config)\n            updated_clients.append((client, new_fit_ins))\n            \n        return updated_clients\n    \n    def configure_evaluate( self, server_round: int, parameters: Parameters, client_manager: ClientManager) -> List[Tuple[ClientProxy, EvaluateIns]]:\n        self.current_round = server_round\n\n        client_evaluate_instructions = super().configure_evaluate(\n            server_round, parameters, client_manager\n        )\n\n        if not client_evaluate_instructions: return []\n\n        available_clients = self._apply_dropout(client_evaluate_instructions, dropout_rate=self.dropout_rate_eval, dropout_pattern=self.dropout_pattern_eval)\n\n        client_ids = [int(client.cid) for client, _ in client_evaluate_instructions]\n        available_client_ids = [int(client.cid) for client, _ in available_clients]\n        dropped_clients = [cid for cid in client_ids if cid not in available_client_ids]\n\n        self.dropped_clients_history_evaluation[server_round] = dropped_clients\n        \n\n        print(f\"Round {server_round}: {len(dropped_clients)} clients dropped out of {len(client_ids)} during evaluation\")\n        print(f\"Dropped client IDs: {dropped_clients}\")\n\n        return available_clients\n\n\n    def _apply_dropout(self, client_instructions: List[Tuple[ClientProxy, Union[FitIns, EvaluateIns ]]], dropout_pattern: str, dropout_rate: 0.3) -> List[Tuple[ClientProxy, FitIns]]:\n        \"\"\"Apply dropout to clients based on the specified pattern.\"\"\"\n        if len(client_instructions) == 0:\n            return []\n\n        # Get all client IDs\n        all_clients = [(client, ins) for client, ins in client_instructions]\n        all_client_ids = [int(client.cid) for client, _ in all_clients]\n\n        # Determine which clients will drop out\n        dropout_mask = [False] * len(all_clients)\n\n        if dropout_pattern == \"random\":\n           \n            for i, cid in enumerate(all_client_ids):\n                \n                if cid in self.fixed_clients:\n                    continue\n            \n                if random.random() < dropout_rate:\n                    dropout_mask[i] = True\n\n        elif dropout_pattern == \"alternate\":\n         \n            if self.current_round % 2 == 1:  \n                for i, cid in enumerate(all_client_ids):\n                    if cid not in self.fixed_clients:\n                        dropout_mask[i] = True\n\n        elif dropout_pattern == \"fixed\":\n      \n            n_dropout = int(len(all_clients) * dropout_rate)\n            for i in range(n_dropout):\n                if all_client_ids[i] not in self.fixed_clients:\n                    dropout_mask[i] = True\n\n        \n        available_clients = [\n            (client, ins) for i, (client, ins) in enumerate(all_clients)\n            if not dropout_mask[i]\n        ]\n\n        return available_clients\n\n    def l2dist(self, p1, p2):\n        return np.linalg.norm(p1 - p2)\n    \n    def weighted_average_oracle(self, points, weights):\n        tot_weights = np.sum(weights)\n        weighted_updates = np.zeros_like(points[0])\n        for w, p in zip(weights, points):\n            weighted_updates += (w / tot_weights) * p\n        return weighted_updates\n    \n    def geometric_median_objective(self, median, points, alphas):\n        return sum([alpha * self.l2dist(median, p) for alpha, p in zip(alphas, points)])\n    \n    def geometric_median_update(self, points, alphas, maxiter=4, eps=1e-5, ftol=1e-6, verbose=False):\n        alphas = np.asarray(alphas, dtype=points[0].dtype) / sum(alphas)\n        median = self.weighted_average_oracle(points, alphas)\n        for _ in range(maxiter):\n            weights = np.asarray([\n                alpha / max(eps, self.l2dist(median, p)) for alpha, p in zip(alphas, points)\n            ], dtype=alphas.dtype)\n            weights /= weights.sum()\n            new_median = self.weighted_average_oracle(points, weights)\n            if abs(self.geometric_median_objective(median, points, alphas) - self.geometric_median_objective(new_median, points, alphas)) < ftol:\n                break\n            median = new_median\n        return median\n\n    def aggregation(self, weights_results, type_aggregation):\n        if type_aggregation == \"mean\":\n            parameters_aggregated = ndarrays_to_parameters(\n                flwr.server.strategy.aggregate.aggregate(weights_results)\n            )\n        elif type_aggregation == \"median\":\n            # get input for RFA\n            update_vectors = [np.concatenate([w.flatten() for w in fl.common.parameters_to_ndarrays(fit_res.parameters)]) for _, fit_res in weights_results]\n            num_examples = [fit_res.num_examples for _, fit_res in weights_results]\n    \n            # aggregation RFA\n            median_vector = self.geometric_median_update(update_vectors, num_examples)\n    \n            parameters = get_parameters(self.net)\n            shapes = [w.shape for w in parameters]\n            sizes = [int(np.prod(s)) for s in shapes]\n            \n            slices = np.split(median_vector, np.cumsum(sizes)[:-1])\n            median_weights = [s.reshape(shape) for s, shape in zip(slices, shapes)]\n            \n            parameters_aggregated = ndarrays_to_parameters(median_weights)\n\n        return parameters_aggregated\n\n    def aggregate_fit(self, server_round: int, results: List[Tuple[ClientProxy, FitRes]], failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]]):\n        \n        # aggregated = super().aggregate_fit(server_round, results, failures)\n        aggregated = self.aggregation(weights_results=results, type_aggregation=\"median\") # params\n\n        if aggregated is not None:\n        # if aggregated and aggregated[0] is not None:\n        #     aggregated_ndarrays: list[np.ndarray] = fl.common.parameters_to_ndarrays(\n        #         aggregated[0]\n        #     )\n\n            # params_dict = zip(self.net.state_dict().keys(), aggregated_ndarrays)\n            aggregated_ndarrays = fl.common.parameters_to_ndarrays(aggregated)\n            params_dict = zip(self.net.state_dict().keys(), aggregated_ndarrays)\n            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n            self.net.load_state_dict(state_dict, strict=True)\n\n            # Save the model to disk\n            torch.save(self.net.state_dict(), f\"model_round_{server_round}.pth\")\n\n        aggregated_metrics = {}\n        if results:\n            metrics = [(res.num_examples, res.metrics) for _, res in results]\n            # update global prototypes\n            for _, res in results:\n                label_counts = pickle.loads(res.metrics[\"label_counts\"])\n                local_prototypes = pickle.loads(res.metrics[\"local_prototypes\"])\n                for label in label_counts:\n                    proto = local_prototypes[label].to(\"cpu\")\n                    count = label_counts[label]\n                    self.global_prototypes[label] += (count * proto) / res.num_examples\n                    \n            aggregated_metrics = self.weighted_average(metrics)\n            self.fit_metrics_history.append(aggregated_metrics)\n\n            if wandb.run is not None:\n                wandb.log({\n                    \"train_server_round\": server_round, \n                    \"train_accuracy\": aggregated_metrics.get(\"train_accuracy\", 0.0), \n                    \"train_loss\" : aggregated_metrics.get(\"train_loss\", 0.0)\n                })\n\n            print(f\"Round {server_round} training metrics: {aggregated_metrics}\")\n\n        return aggregated, aggregated_metrics\n\n    def aggregate_evaluate( self, server_round: int, results: List[Tuple[ClientProxy, EvaluateRes]],  failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]]):\n        \n        aggregated = super().aggregate_evaluate(server_round, results, failures)\n\n        if results:\n            metrics = [(res.num_examples, res.metrics) for _, res in results]\n            aggregated_metrics = self.weighted_average(metrics)\n            self.eval_metrics_history.append(aggregated_metrics)\n            \n            if wandb.run is not None:\n                wandb.log({\n                    \"eval_server_round\" : server_round,\n                    \"test_accuracy\": aggregated_metrics.get(\"test_accuracy\"), \n                    \"test_loss\" : aggregated_metrics.get(\"test_loss\"), \n                })\n                \n            print(f\"Round {server_round} evaluation metrics: {aggregated_metrics}\")\n\n        return aggregated\n\n    def get_dropout_history(self) -> Dict[int, List[int]]:\n        return self.dropped_clients_history_training, self.dropped_clients_history_evaluation\n\n    def get_metrics_history(self) -> Tuple[List[Dict[str, float]], List[Dict[str, float]]]: \n        return self.fit_metrics_history, self.eval_metrics_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.420857Z","iopub.execute_input":"2025-05-21T18:21:11.421045Z","iopub.status.idle":"2025-05-21T18:21:11.453482Z","shell.execute_reply.started":"2025-05-21T18:21:11.421032Z","shell.execute_reply":"2025-05-21T18:21:11.452843Z"}},"outputs":[],"execution_count":234},{"cell_type":"code","source":"from flwr.client import ClientApp\nfrom flwr.server import ServerApp\nfrom flwr.simulation import run_simulation\nimport os\nfrom flwr.common import Context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.454205Z","iopub.execute_input":"2025-05-21T18:21:11.454478Z","iopub.status.idle":"2025-05-21T18:21:11.471703Z","shell.execute_reply.started":"2025-05-21T18:21:11.454454Z","shell.execute_reply":"2025-05-21T18:21:11.471125Z"}},"outputs":[],"execution_count":235},{"cell_type":"code","source":"def run_dropout_experiment(\n    client_fn_creator,\n    pl_model : Union[pl.LightningModule, torch.nn.Module], \n    num_clients: int,\n    num_rounds: int = 5,\n    dropout_rate_training: float = 0.3,\n    dropout_rate_eval: float = 0.3,\n    dropout_pattern_train: str = \"random\",\n    dropout_pattern_eval: str = \"random\",\n    fixed_clients: Optional[List[int]] = None,\n    experiment_name: str = \"dropout_experiment\",\n    save_dir: str = \"model_weights\",\n    num_gpus : int = 0, \n    resource_config : Optional[Dict[str, float]] = None,\n\n):\n    \n    # Configure client app\n    print(f\"\\nStarting experiment: {experiment_name}\")\n    print(f\"Dropout rate training: {dropout_rate_training}, Pattern: {dropout_pattern_train}\")\n    print(f\"Dropout rate evaluation: {dropout_rate_eval}, Pattern: {dropout_pattern_eval   }\")\n    print(f\"Number of GPUs: {num_gpus}\")\n    print(f\"Number of clients: {num_clients}\")\n    print(f\"Number of rounds: {num_rounds}\")\n    print(f\"Fixed clients: {fixed_clients or []}\")\n    # Create strategy with dropout\n    strategy = DropoutFedAvg(\n        net=pl_model.model if isinstance(pl_model, pl.LightningModule) else pl_model,\n        dropout_rate_training=dropout_rate_training,\n        dropout_rate_eval=dropout_rate_eval,\n        dropout_pattern_train=dropout_pattern_train,\n        dropout_pattern_eval=dropout_pattern_eval,\n        fixed_clients=fixed_clients or [],\n        fraction_fit=1.0,\n        fraction_evaluate=1.0,\n        min_fit_clients=1,\n        min_evaluate_clients=1,\n        min_available_clients=1,\n\n    )\n\n    # Configure server with strategy\n    def server_fn(server_context: Context):\n        from flwr.server import ServerAppComponents, ServerConfig\n        config = ServerConfig(num_rounds=num_rounds)\n        return ServerAppComponents(strategy=strategy, config=config)\n\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    epochs = resource_config.get(\"epochs\", 1) if resource_config else 1\n    client_datasets = resource_config.get(\"client_datasets\", {}) if resource_config else {}\n\n    \n    batch_size = resource_config.get(\"batch_size\", 32) if resource_config else 32\n    learning_rate = resource_config.get(\"learning_rate\", 0.001) if resource_config else 0.001\n    num_workers = resource_config.get(\"num_workers\", 1) if resource_config else 1\n    client_fn = client_fn_creator(device=device, epochs=epochs, client_datasets=client_datasets\n                                , batch_size=batch_size, pl_model=pl_model, num_workers=num_workers)\n    \n    # Create client and server apps\n    client_app = ClientApp(client_fn=client_fn)\n    server_app = ServerApp(server_fn=server_fn)\n\n    # Configure backend\n    backend_config = {\n        \"client_resources\": {\n            \"num_cpus\": 1,\n            \"num_gpus\": num_gpus,\n        }\n    }\n    history = strategy.get_dropout_history()\n    # Run simulation\n    try:\n        run_simulation(\n            client_app=client_app,\n            server_app=server_app,\n            num_supernodes=num_clients,\n            backend_config=backend_config,\n        )\n\n        # Get metrics directly from strategy\n        fit_metrics, eval_metrics = strategy.get_metrics_history()\n\n        # Format metrics for plotting\n        rounds = list(range(1, len(eval_metrics) + 1))\n\n        train_accuracy_values = [metrics.get(\"train_accuracy\", 0.0) for metrics in fit_metrics]\n        train_loss_values = [metrics.get(\"train_loss\", 0.0) for metrics in fit_metrics]\n        \n\n        test_accuracy_values = [metrics.get(\"test_accuracy\", 0.0) for metrics in eval_metrics]\n        test_loss_values = [metrics.get(\"test_loss\", 0.0) for metrics in eval_metrics]\n\n\n        # cleanup_wandb_loggers()\n        results = {\n            \"rounds\": rounds,\n            \"train_accuracy\": train_accuracy_values,\n            \"train_loss\": train_loss_values,\n            \"test_accuracy\": test_accuracy_values,\n            \"test_loss\": test_loss_values,\n        }\n\n        return results, history\n    \n    except Exception as e:\n        print(f\"Error in dropout experiment: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {\"error\": str(e)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.472483Z","iopub.execute_input":"2025-05-21T18:21:11.472770Z","iopub.status.idle":"2025-05-21T18:21:11.487561Z","shell.execute_reply.started":"2025-05-21T18:21:11.472748Z","shell.execute_reply":"2025-05-21T18:21:11.486991Z"}},"outputs":[],"execution_count":236},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"import hydra \nfrom omegaconf import DictConfig, OmegaConf\nimport logging \nimport wandb\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.488146Z","iopub.execute_input":"2025-05-21T18:21:11.488317Z","iopub.status.idle":"2025-05-21T18:21:11.505126Z","shell.execute_reply.started":"2025-05-21T18:21:11.488304Z","shell.execute_reply":"2025-05-21T18:21:11.504492Z"}},"outputs":[],"execution_count":237},{"cell_type":"markdown","source":"## Config ","metadata":{}},{"cell_type":"code","source":"config = {\n    \"base_path\": \"/kaggle\",\n    \"device\": \"cuda\",  # from train.device\n    \"run_name\": \"dropout_30pct_fixed_train_fixed_eval_0.3_fixed_0.3\",  # resolved using experiment values\n    \"seed\": 42,\n    \"num_clients\": 4,\n    \"num_rounds\": 3,\n    \"gpus\": 1,\n    \"wandb_log\": True,\n    \"train\": {\n        \"batch_size\": 4,\n        \"learning_rate\": 0.001,\n        \"epochs\": 5,\n        \"device\": \"cuda\",\n        \"num_workers\": 2,\n        \"weight_decay\": 0.0001,\n        \"scheduler\": {\n            \"use\": False,\n            \"type\": \"cosine\",\n            \"warmup_epochs\": 5,\n            \"min_lr\": 0.0001,\n        }\n    },\n\n    \"experiment\": {\n        \"pattern_train\": \"fixed\",\n        \"pattern_eval\": \"fixed\",\n        \"dropout_rate_training\": 0.3,\n        \"dropout_rate_eval\": 0.3,\n        \"fixed_clients\": [0, 1, 2],\n        \"name\": \"dropout_30pct_fixed_train_fixed_eval\",\n    },\n\n    \"data\": {\n        \"root_path\": \"/kaggle/input/mri-dataset/datasetzip/not_skull_stripped\",\n        \"label_path\": \"/kaggle/input/mri-label/label.csv\",\n        \"val_ratio\": 0.2,\n        \"overlap_ratio\": 0.2,\n        \"distribution\": \"same\",\n    },\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.505758Z","iopub.execute_input":"2025-05-21T18:21:11.506019Z","iopub.status.idle":"2025-05-21T18:21:11.520510Z","shell.execute_reply.started":"2025-05-21T18:21:11.505995Z","shell.execute_reply":"2025-05-21T18:21:11.519787Z"}},"outputs":[],"execution_count":238},{"cell_type":"code","source":"def run_experiment_with_lightning(cfg_dict: dict) -> None:\n    cfg: DictConfig = OmegaConf.create(cfg_dict)  # convert to DictConfig to retain dot-access\n    logger =  logger = logging.getLogger(__name__)\n    logger.info(f\"Running experiment with config: {cfg.experiment.name}\")\n    logger.info(f\"Config: {OmegaConf.to_yaml(cfg)}\")\n\n    if cfg.wandb_log:\n        wandb.init(\n            project=\"sex-classification\",\n            name=f\"{cfg.experiment.name}\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            group=\"server\"\n        )\n\n    device = cfg.device\n    epochs = cfg.train.epochs\n\n    logger.info(\"Loading model\")\n    net: nn.Module = DenseNet()\n    pl_model: pl.LightningModule = DenseNetModule(net = net)\n\n    logger.info(\"Loading dataset\")\n    is_3d = True if isinstance(pl_model, DenseNetModule) else False\n    print(f\"Is 3D: {is_3d}\")\n\n    full_dataset = MRIDataset(\n        root_dir=cfg.data.root_path,\n        label_path=cfg.data.label_path,\n        is_3d=is_3d\n    )\n\n    logger.info(f\"Dataset loaded successfully with len is {len(full_dataset)}\")\n    logger.info(f\"Splitting dataset into {cfg.num_clients} clients\")\n\n    if cfg.data.distribution == \"iid\":\n        client_datasets = iid_client_split(\n            full_dataset,\n            num_client=cfg.num_clients,\n            val_ratio=cfg.data.val_ratio\n        )\n    elif cfg.data.distribution == \"same\":\n        client_datasets = same_distribution_client_split(\n            full_dataset,\n            num_client=cfg.num_clients,\n            val_ratio=cfg.data.val_ratio,\n            overlap_ratio=cfg.data.overlap_ratio,\n            root_dir=cfg.data.root_path,\n            is_3d=is_3d\n        )\n    else:\n        raise ValueError(f\"Unknown distribution type: {cfg.data.distribution}\")\n\n    logger.info(f\"Client datasets created successfully with {len(client_datasets)} clients\")\n\n    resources = {\n        \"client_datasets\": client_datasets,\n        \"device\": device,\n        \"epochs\": epochs,\n        \"batch_size\": cfg.train.batch_size,\n        \"learning_rate\": cfg.train.learning_rate,\n        \"num_workers\": cfg.train.num_workers\n    }\n\n    logger.info(\"Running experiments with PyTorch Lightning\")\n\n    results, history = run_dropout_experiment(\n        client_fn_creator=create_lightning_client_fn,\n        pl_model=pl_model,\n        num_clients=cfg.num_clients,\n        num_rounds=cfg.num_rounds,\n        dropout_rate_training=cfg.experiment.dropout_rate_training,\n        dropout_rate_eval=cfg.experiment.dropout_rate_eval,\n        dropout_pattern_train=cfg.experiment.pattern_train,\n        dropout_pattern_eval=cfg.experiment.pattern_eval,\n        experiment_name=cfg.experiment.name,\n        num_gpus=cfg.gpus,\n        resource_config=resources\n    )\n\n    logger.info(\"Run successfully + wandb tracking\")\n    print(f\"Result is {results}\")\n\n    if cfg.wandb_log:\n        wandb.finish()\n\n    logger.info(\"Experiments completed successfully\")\n    logger.info(f\"Client Dropout History: {history}\")\n\n    return results, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.521316Z","iopub.execute_input":"2025-05-21T18:21:11.521620Z","iopub.status.idle":"2025-05-21T18:21:11.541215Z","shell.execute_reply.started":"2025-05-21T18:21:11.521603Z","shell.execute_reply":"2025-05-21T18:21:11.540666Z"}},"outputs":[],"execution_count":239},{"cell_type":"code","source":"wandb.login(key=WANDB_APIKEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.541845Z","iopub.execute_input":"2025-05-21T18:21:11.542094Z","iopub.status.idle":"2025-05-21T18:21:11.559679Z","shell.execute_reply.started":"2025-05-21T18:21:11.542078Z","shell.execute_reply":"2025-05-21T18:21:11.559148Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":240,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":240},{"cell_type":"code","source":"run_experiment_with_lightning(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:21:11.560639Z","iopub.execute_input":"2025-05-21T18:21:11.560820Z","iopub.status.idle":"2025-05-21T20:01:32.243535Z","shell.execute_reply.started":"2025-05-21T18:21:11.560807Z","shell.execute_reply":"2025-05-21T20:01:32.242719Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dropout_30pct_fixed_train_fixed_eval</strong> at: <a href='https://wandb.ai/iai-uet-vnu/sex-classification/runs/56n93um2' target=\"_blank\">https://wandb.ai/iai-uet-vnu/sex-classification/runs/56n93um2</a><br> View project at: <a href='https://wandb.ai/iai-uet-vnu/sex-classification' target=\"_blank\">https://wandb.ai/iai-uet-vnu/sex-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_173611-56n93um2/logs</code>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py:259: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n  self.scope.user = {\"email\": email}  # noqa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_182111-evrc7sei</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iai-uet-vnu/sex-classification/runs/evrc7sei' target=\"_blank\">dropout_30pct_fixed_train_fixed_eval</a></strong> to <a href='https://wandb.ai/iai-uet-vnu/sex-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iai-uet-vnu/sex-classification' target=\"_blank\">https://wandb.ai/iai-uet-vnu/sex-classification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iai-uet-vnu/sex-classification/runs/evrc7sei' target=\"_blank\">https://wandb.ai/iai-uet-vnu/sex-classification/runs/evrc7sei</a>"},"metadata":{}},{"name":"stdout","text":"Is 3D: True\n","output_type":"stream"},{"name":"stderr","text":"\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n\u001b[92mINFO \u001b[0m:      \n\u001b[92mINFO \u001b[0m:      [INIT]\n\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n","output_type":"stream"},{"name":"stdout","text":"\nStarting experiment: dropout_30pct_fixed_train_fixed_eval\nDropout rate training: 0.3, Pattern: fixed\nDropout rate evaluation: 0.3, Pattern: fixed\nNumber of GPUs: 1\nNumber of clients: 4\nNumber of rounds: 3\nFixed clients: []\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(pid=23331)\u001b[0m 2025-05-21 18:26:41.255211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n\u001b[36m(pid=23331)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n\u001b[36m(pid=23331)\u001b[0m E0000 00:00:1747852001.284175   23331 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n\u001b[36m(pid=23331)\u001b[0m E0000 00:00:1747852001.292346   23331 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n\u001b[92mINFO \u001b[0m:      \n\u001b[92mINFO \u001b[0m:      [ROUND 1]\n\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 4)\n","output_type":"stream"},{"name":"stdout","text":"Round 1: 1 clients dropped out of 4 during training\nDropped client IDs: [17353880090414495932]\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_0 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_2 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_3 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 4)\n","output_type":"stream"},{"name":"stdout","text":"Round 1 training metrics: {'train_loss': 0.625308164996964, 'train_accuracy': 0.6459330149244463, 'val_loss': 0.8320158379214803, 'val_accuracy': 0.5482771422304035}\nRound 1: 1 clients dropped out of 4 during evaluation\nDropped client IDs: [17353880090414495932]\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.46048110723495483    │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    0.8108912110328674     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.5463917255401611     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    0.7658227682113647     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n\u001b[92mINFO \u001b[0m:      \n\u001b[92mINFO \u001b[0m:      [ROUND 2]\n\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 4)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.35177865624427795    │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │     0.706638514995575     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\nRound 1 evaluation metrics: {'test_accuracy': 0.4574850231944444, 'test_loss': 0.7635967811424575}\nRound 2: 1 clients dropped out of 4 during training\nDropped client IDs: [17353880090414495932]\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_0 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_2 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_3 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 4)\n","output_type":"stream"},{"name":"stdout","text":"Round 2 training metrics: {'train_loss': 0.5748761412487076, 'train_accuracy': 0.7141148220551642, 'val_loss': 1.3249151058601991, 'val_accuracy': 0.5797842055131374}\nRound 2: 1 clients dropped out of 4 during evaluation\nDropped client IDs: [11251355939264284581]\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.4982817769050598     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    1.2447819709777832     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.4845360815525055     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    1.3109047412872314     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n\u001b[92mINFO \u001b[0m:      \n\u001b[92mINFO \u001b[0m:      [ROUND 3]\n\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 4)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.6996047496795654     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    0.7597509622573853     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\nRound 2 evaluation metrics: {'test_accuracy': 0.5544910161438102, 'test_loss': 1.1208644631380094}\nRound 3: 1 clients dropped out of 4 during training\nDropped client IDs: [3199048786198958087]\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_0 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_1 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints/client_2 exists and is not empty.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m   | Name           | Type               | Params | Mode \n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0 | model          | DenseNet           | 1.7 M  | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1 | criterion      | CrossEntropyLoss   | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 2 | nll_loss       | NLLLoss            | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 3 | train_accuracy | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 4 | val_accuracy   | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 5 | test_accuracy  | MulticlassAccuracy | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6 | rbsm           | RelaxedBSM         | 0      | train\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m --------------------------------------------------------------\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Non-trainable params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 1.7 M     Total params\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 6.797     Total estimated model params size (MB)\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 313       Modules in train mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m 0         Modules in eval mode\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 4)\n","output_type":"stream"},{"name":"stdout","text":"Round 3 training metrics: {'train_loss': 0.48577622572580975, 'train_accuracy': 0.781214197476705, 'val_loss': 0.9182385404904684, 'val_accuracy': 0.5738831559816996}\nRound 3: 1 clients dropped out of 4 during evaluation\nDropped client IDs: [17353880090414495932]\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.6048110127449036     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    0.6313678622245789     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.5326460599899292     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    0.7016363739967346     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: GPU available: True (cuda), used: True\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: TPU available: False, using: 0 TPU cores\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: HPU available: False, using: 0 HPUs\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n\u001b[92mINFO \u001b[0m:      \n\u001b[92mINFO \u001b[0m:      [SUMMARY]\n\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 5673.61s\n\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.7635967811424575\n\u001b[92mINFO \u001b[0m:      \t\tround 2: 1.1208644631380094\n\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.6037026001307779\n\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, fit):\n\u001b[92mINFO \u001b[0m:      \t{'train_accuracy': [(1, 0.6459330149244463),\n\u001b[92mINFO \u001b[0m:      \t                    (2, 0.7141148220551642),\n\u001b[92mINFO \u001b[0m:      \t                    (3, 0.781214197476705)],\n\u001b[92mINFO \u001b[0m:      \t 'train_loss': [(1, 0.625308164996964),\n\u001b[92mINFO \u001b[0m:      \t                (2, 0.5748761412487076),\n\u001b[92mINFO \u001b[0m:      \t                (3, 0.48577622572580975)],\n\u001b[92mINFO \u001b[0m:      \t 'val_accuracy': [(1, 0.5482771422304035),\n\u001b[92mINFO \u001b[0m:      \t                  (2, 0.5797842055131374),\n\u001b[92mINFO \u001b[0m:      \t                  (3, 0.5738831559816996)],\n\u001b[92mINFO \u001b[0m:      \t 'val_loss': [(1, 0.8320158379214803),\n\u001b[92mINFO \u001b[0m:      \t              (2, 1.3249151058601991),\n\u001b[92mINFO \u001b[0m:      \t              (3, 0.9182385404904684)]}\n\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n\u001b[92mINFO \u001b[0m:      \t{'test_accuracy': [(1, 0.4574850231944444),\n\u001b[92mINFO \u001b[0m:      \t                   (2, 0.5544910161438102),\n\u001b[92mINFO \u001b[0m:      \t                   (3, 0.6167664834125314)],\n\u001b[92mINFO \u001b[0m:      \t 'test_loss': [(1, 0.7635967811424575),\n\u001b[92mINFO \u001b[0m:      \t               (2, 1.1208644631380094),\n\u001b[92mINFO \u001b[0m:      \t               (3, 0.6037026001307779)]}\n\u001b[92mINFO \u001b[0m:      \n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/acc          │    0.7272727489471436     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m │         test/loss         │    0.4592388868331909     │\n\u001b[36m(ClientAppActor pid=23331)\u001b[0m └───────────────────────────┴───────────────────────────┘\nRound 3 evaluation metrics: {'test_accuracy': 0.6167664834125314, 'test_loss': 0.6037026001307779}\nResult is {'rounds': [1, 2, 3], 'train_accuracy': [0.6459330149244463, 0.7141148220551642, 0.781214197476705], 'train_loss': [0.625308164996964, 0.5748761412487076, 0.48577622572580975], 'test_accuracy': [0.4574850231944444, 0.5544910161438102, 0.6167664834125314], 'test_loss': [0.7635967811424575, 1.1208644631380094, 0.6037026001307779]}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_server_round</td><td>▁▅█</td></tr><tr><td>test_accuracy</td><td>▁▅█</td></tr><tr><td>test_loss</td><td>▃█▁</td></tr><tr><td>train_accuracy</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▅▁</td></tr><tr><td>train_server_round</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_server_round</td><td>3</td></tr><tr><td>test_accuracy</td><td>0.61677</td></tr><tr><td>test_loss</td><td>0.6037</td></tr><tr><td>train_accuracy</td><td>0.78121</td></tr><tr><td>train_loss</td><td>0.48578</td></tr><tr><td>train_server_round</td><td>3</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dropout_30pct_fixed_train_fixed_eval</strong> at: <a href='https://wandb.ai/iai-uet-vnu/sex-classification/runs/evrc7sei' target=\"_blank\">https://wandb.ai/iai-uet-vnu/sex-classification/runs/evrc7sei</a><br> View project at: <a href='https://wandb.ai/iai-uet-vnu/sex-classification' target=\"_blank\">https://wandb.ai/iai-uet-vnu/sex-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_182111-evrc7sei/logs</code>"},"metadata":{}},{"execution_count":241,"output_type":"execute_result","data":{"text/plain":"({'rounds': [1, 2, 3],\n  'train_accuracy': [0.6459330149244463,\n   0.7141148220551642,\n   0.781214197476705],\n  'train_loss': [0.625308164996964, 0.5748761412487076, 0.48577622572580975],\n  'test_accuracy': [0.4574850231944444,\n   0.5544910161438102,\n   0.6167664834125314],\n  'test_loss': [0.7635967811424575, 1.1208644631380094, 0.6037026001307779]},\n ({1: [17353880090414495932],\n   2: [17353880090414495932],\n   3: [3199048786198958087]},\n  {1: [17353880090414495932],\n   2: [11251355939264284581],\n   3: [17353880090414495932]}))"},"metadata":{}}],"execution_count":241}]}